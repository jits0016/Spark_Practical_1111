{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6909559d-3d6b-4714-9ce1-c7c7c7755636",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3792435141996747#setting/sparkui/0427-033850-chgs5kgl/driver-8837482409791654161\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3792435141996747#setting/sparkui/0427-033850-chgs5kgl/driver-8837482409791654161\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d92c11ad-d628-4b93-878d-9fb2f002ee41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|              _c0|                _c1|  _c2|\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\").option(\"inferschema\",\"false\")\\\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f207f4-6a4d-4d0e-b7f2-6d826da73188",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_header_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\").option(\"inferschema\",\"false\")\\\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "flight_header_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c8133a-2f3d-4493-87b0-87bf16cc96a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_header_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f51aba-044b-4d53-9e85-813286780c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_header_schema_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\").option(\"inferschema\",\"true\")\\\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "flight_header_schema_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce24c8b6-be4d-4878-9624-4c8d45d0de03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_header_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5e8f20-dd0a-43a8-9a54-299812eb613a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2929419749642601>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m flight_define_shema_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferschema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      3\u001B[0m         \u001B[38;5;241m.\u001B[39moption(my_schema)\\\n",
       "\u001B[1;32m      4\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      5\u001B[0m             \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/2010_summary.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m flight_define_schema_df\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: option() missing 1 required positional argument: 'value'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2929419749642601>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m flight_define_shema_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferschema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;241m.\u001B[39moption(my_schema)\\\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      5\u001B[0m             \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/2010_summary.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m flight_define_schema_df\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\n\u001B[0;31mTypeError\u001B[0m: option() missing 1 required positional argument: 'value'",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: option() missing 1 required positional argument: 'value'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flight_define_shema_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\").option(\"inferschema\",\"false\")\\\n",
    "        .option(my_schema)\\\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "flight_define_schema_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9b5bc4-62d9-413a-8a44-223a37130898",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StructType,StringType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5779e52-60a6-4987-b3b5-f14bdc93d2fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = StructType(\n",
    "                       [\n",
    "\t\t\t\t\t    StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "\t\t\t\t\t\tStructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "\t\t\t\t\t\tStructField(\"count\",IntegerType(),True)\n",
    "\t\t\t\t\t\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c24e762-6511-48ef-95a7-1373e7995e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "f_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"skipRows\",1)\\\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "    .schema(my_schema)\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .load(\"/FileStore/2010_summary.csv\")\n",
    "\n",
    "f_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea93638-961b-46a5-93ba-11e80955ef76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------+\n| id|    name|age|salary|     address| nominee|     _c6|\n+---+--------+---+------+------------+--------+--------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|    null|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|    null|\n|  3|  Pritam| 22|150000|   Bangalore|   India|nominee3|\n|  4|Prantosh| 17|200000|     Kolkata|   India|nominee4|\n|  5|  Vikash| 31|300000|        null|nominee5|    null|\n+---+--------+---+------+------------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_p_df = spark.read.format(\"CSV\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .load(\"/FileStore/emp.csv\")\n",
    "\n",
    "emp_p_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131c7199-fd70-41b1-8f0d-5270448bb8a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n|id,name,age,salary,address,nominee|\n+----------------------------------+\n|              1,Manish,26,75000...|\n|              2,Nikita,23,10000...|\n|              3,Pritam,22,15000...|\n|              4,Prantosh,17,200...|\n|              5,Vikash,31,30000...|\n+----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_f_df = spark.read.format(\"CSV\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"mode\",\"FAILFAST\")\\\n",
    "    .load(\"/FileStore/emp_df.csv\")\n",
    "\n",
    "emp_f_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185e65af-f3dc-4b35-89b9-617fe5323286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_schema = StructType([StructField(\"id\",IntegerType(),True),\n",
    "                         StructField(\"name\",StringType(),True),\n",
    "                         StructField(\"age\",IntegerType(),True),\n",
    "                         StructField(\"Salary\",IntegerType(),True),\n",
    "                         StructField(\"address\",StringType(),True),\n",
    "                         StructField(\"nominee\",StringType(),True),\n",
    "                        StructField(\"_corrupt_record\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e8695e-b786-4410-ac45-7cfe043073f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------------+\n| id|    name|age|Salary|     address| nominee|     _corrupt_record|\n+---+--------+---+------+------------+--------+--------------------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|1,Manish,26,75000...|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|2,Nikita,23,10000...|\n|  3|  Pritam| 22|150000|   Bangalore|   India|3,Pritam,22,15000...|\n|  4|Prantosh| 17|200000|     Kolkata|   India|4,Prantosh,17,200...|\n|  5|  Vikash| 31|300000|        null|nominee5|5,Vikash,31,30000...|\n+---+--------+---+------+------------+--------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .schema(emp_schema)\\\n",
    "    .load(\"/FileStore/emp.csv\")\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88674a4c-df0a-4c96-b453-ba45f3d33006",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n|id |name    |age|Salary|address     |nominee |_corrupt_record                            |\n+---+--------+---+------+------------+--------+-------------------------------------------+\n|1  |Manish  |26 |75000 |bihar       |nominee1|1,Manish,26,75000,bihar,nominee1,          |\n|2  |Nikita  |23 |100000|uttarpradesh|nominee2|2,Nikita,23,100000,uttarpradesh,nominee2,  |\n|3  |Pritam  |22 |150000|Bangalore   |India   |3,Pritam,22,150000,Bangalore,India,nominee3|\n|4  |Prantosh|17 |200000|Kolkata     |India   |4,Prantosh,17,200000,Kolkata,India,nominee4|\n|5  |Vikash  |31 |300000|null        |nominee5|5,Vikash,31,300000,,nominee5,              |\n+---+--------+---+------+------------+--------+-------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .schema(emp_schema)\\\n",
    "    .load(\"/FileStore/emp.csv\")\n",
    "\n",
    "emp_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938b9a7f-6259-4c52-affa-2da3879ef16d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+-------+-------+---------------+\n|id |name|age|Salary|address|nominee|_corrupt_record|\n+---+----+---+------+-------+-------+---------------+\n+---+----+---+------+-------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .schema(emp_schema)\\\n",
    "    .option(\"badRecordsPath\",\"/FileStore/tables/bad_recods\")\\\n",
    "    .load(\"/FileStore/emp.csv\")\n",
    "\n",
    "emp_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3f59d4f-cf38-48de-8116-b471124da953",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/bad_recods/20240410T165043/bad_records/part-00000-5c197f75-4066-462f-b7f6-c8d15e3a0ed2</td><td>part-00000-5c197f75-4066-462f-b7f6-c8d15e3a0ed2</td><td>1098</td><td>1712767845000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/bad_recods/20240410T165043/bad_records/part-00000-5c197f75-4066-462f-b7f6-c8d15e3a0ed2",
         "part-00000-5c197f75-4066-462f-b7f6-c8d15e3a0ed2",
         1098,
         1712767845000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls \n",
    "dbfs:/FileStore/tables/bad_recods/20240410T165043/bad_records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c854cc4a-f34f-4f41-8d47-e304fa6e5237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n|path                   |reason                                                                                                                          |record                                     |\n+-----------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n|dbfs:/FileStore/emp.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 1,Manish,26,75000,bihar,nominee1,          |1,Manish,26,75000,bihar,nominee1,          |\n|dbfs:/FileStore/emp.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 2,Nikita,23,100000,uttarpradesh,nominee2,  |2,Nikita,23,100000,uttarpradesh,nominee2,  |\n|dbfs:/FileStore/emp.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3|3,Pritam,22,150000,Bangalore,India,nominee3|\n|dbfs:/FileStore/emp.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 4,Prantosh,17,200000,Kolkata,India,nominee4|4,Prantosh,17,200000,Kolkata,India,nominee4|\n|dbfs:/FileStore/emp.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 5,Vikash,31,300000,,nominee5,              |5,Vikash,31,300000,,nominee5,              |\n+-----------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "bad_data_df = spark.read.format(\"json\").load(\"/FileStore/tables/bad_recods/20240410T165043/bad_records/part-00000-5c197f75-4066-462f-b7f6-c8d15e3a0ed2\")\n",
    "\n",
    "bad_data_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7778a5f-c599-44c2-8fe4-1bb84ee5e8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/spark/pratical/10000Records.csv</td><td>10000Records.csv</td><td>2775309</td><td>1713163917000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/Country_Code.xlsx</td><td>Country_Code.xlsx</td><td>8783</td><td>1712771647000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/ETH_1H.csv</td><td>ETH_1H.csv</td><td>2675166</td><td>1713163920000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/ETH_USD.csv</td><td>ETH_USD.csv</td><td>30306</td><td>1713163919000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/Multi_line_correct.json</td><td>Multi_line_correct.json</td><td>622</td><td>1712769816000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/Multi_line_incorrect.json</td><td>Multi_line_incorrect.json</td><td>1278</td><td>1712769816000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/aadharpancarddata.csv</td><td>aadharpancarddata.csv</td><td>3276</td><td>1713163907000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/bank_full.csv</td><td>bank_full.csv</td><td>4610348</td><td>1713163918000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/corrupted_json.json</td><td>corrupted_json.json</td><td>438</td><td>1712769816000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/date_differ_timezone.csv</td><td>date_differ_timezone.csv</td><td>5424</td><td>1713163917000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/datemultiformat.csv</td><td>datemultiformat.csv</td><td>691</td><td>1713163918000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/emp_details.csv</td><td>emp_details.csv</td><td>686</td><td>1712828417000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file1.json</td><td>file1.json</td><td>2198732</td><td>1712771658000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file2.json</td><td>file2.json</td><td>20006072</td><td>1712771670000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file3.json</td><td>file3.json</td><td>15610388</td><td>1712771670000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file4.json</td><td>file4.json</td><td>16427749</td><td>1712771684000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file5.json</td><td>file5.json</td><td>669503</td><td>1712771672000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/flipkartdelivary.csv</td><td>flipkartdelivary.csv</td><td>1648</td><td>1713163919000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/healthdata.csv</td><td>healthdata.csv</td><td>343</td><td>1713163920000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/line_delimited_json.json</td><td>line_delimited_json.json</td><td>440</td><td>1712770674000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet</td><td>part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet</td><td>3921</td><td>1712821856000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/single_file_json_with_extra_fields.json</td><td>single_file_json_with_extra_fields.json</td><td>470</td><td>1712769816000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/tourist_Journey_data.csv</td><td>tourist_Journey_data.csv</td><td>7072</td><td>1713163920000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/us_500.csv</td><td>us_500.csv</td><td>92320</td><td>1713163920000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/world_bank.json</td><td>world_bank.json</td><td>2969043</td><td>1713163922000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/zomato.csv</td><td>zomato.csv</td><td>2257316</td><td>1712771676000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/spark/pratical/10000Records.csv",
         "10000Records.csv",
         2775309,
         1713163917000
        ],
        [
         "dbfs:/FileStore/spark/pratical/Country_Code.xlsx",
         "Country_Code.xlsx",
         8783,
         1712771647000
        ],
        [
         "dbfs:/FileStore/spark/pratical/ETH_1H.csv",
         "ETH_1H.csv",
         2675166,
         1713163920000
        ],
        [
         "dbfs:/FileStore/spark/pratical/ETH_USD.csv",
         "ETH_USD.csv",
         30306,
         1713163919000
        ],
        [
         "dbfs:/FileStore/spark/pratical/Multi_line_correct.json",
         "Multi_line_correct.json",
         622,
         1712769816000
        ],
        [
         "dbfs:/FileStore/spark/pratical/Multi_line_incorrect.json",
         "Multi_line_incorrect.json",
         1278,
         1712769816000
        ],
        [
         "dbfs:/FileStore/spark/pratical/aadharpancarddata.csv",
         "aadharpancarddata.csv",
         3276,
         1713163907000
        ],
        [
         "dbfs:/FileStore/spark/pratical/bank_full.csv",
         "bank_full.csv",
         4610348,
         1713163918000
        ],
        [
         "dbfs:/FileStore/spark/pratical/corrupted_json.json",
         "corrupted_json.json",
         438,
         1712769816000
        ],
        [
         "dbfs:/FileStore/spark/pratical/date_differ_timezone.csv",
         "date_differ_timezone.csv",
         5424,
         1713163917000
        ],
        [
         "dbfs:/FileStore/spark/pratical/datemultiformat.csv",
         "datemultiformat.csv",
         691,
         1713163918000
        ],
        [
         "dbfs:/FileStore/spark/pratical/emp_details.csv",
         "emp_details.csv",
         686,
         1712828417000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file1.json",
         "file1.json",
         2198732,
         1712771658000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file2.json",
         "file2.json",
         20006072,
         1712771670000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file3.json",
         "file3.json",
         15610388,
         1712771670000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file4.json",
         "file4.json",
         16427749,
         1712771684000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file5.json",
         "file5.json",
         669503,
         1712771672000
        ],
        [
         "dbfs:/FileStore/spark/pratical/flipkartdelivary.csv",
         "flipkartdelivary.csv",
         1648,
         1713163919000
        ],
        [
         "dbfs:/FileStore/spark/pratical/healthdata.csv",
         "healthdata.csv",
         343,
         1713163920000
        ],
        [
         "dbfs:/FileStore/spark/pratical/line_delimited_json.json",
         "line_delimited_json.json",
         440,
         1712770674000
        ],
        [
         "dbfs:/FileStore/spark/pratical/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet",
         "part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet",
         3921,
         1712821856000
        ],
        [
         "dbfs:/FileStore/spark/pratical/single_file_json_with_extra_fields.json",
         "single_file_json_with_extra_fields.json",
         470,
         1712769816000
        ],
        [
         "dbfs:/FileStore/spark/pratical/tourist_Journey_data.csv",
         "tourist_Journey_data.csv",
         7072,
         1713163920000
        ],
        [
         "dbfs:/FileStore/spark/pratical/us_500.csv",
         "us_500.csv",
         92320,
         1713163920000
        ],
        [
         "dbfs:/FileStore/spark/pratical/world_bank.json",
         "world_bank.json",
         2969043,
         1713163922000
        ],
        [
         "dbfs:/FileStore/spark/pratical/zomato.csv",
         "zomato.csv",
         2257316,
         1712771676000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/spark/pratical/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87155aee-2511-40a5-abd5-906009fde20c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+--------+------+\n|_corrupt_record|age |name    |salary|\n+---------------+----+--------+------+\n|null           |20  |Manish  |20000 |\n|\u0000              |null|null    |null  |\n|null           |25  |Nikita  |21000 |\n|\u0000              |null|null    |null  |\n|null           |16  |Pritam  |22000 |\n|\u0000              |null|null    |null  |\n|null           |35  |Prantosh|25000 |\n|\u0000              |null|null    |null  |\n|null           |67  |Vikash  |40000 |\n+---------------+----+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "\t.option(\"mode\",\"PERMISSIVE\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/line_delimited_json.json\")\\\n",
    "\t.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14cdc512-36e2-439e-984f-ed0aba7554e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3197560269081128>:5\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    .show(truncate=False)\u001B[0m\n",
       "\u001B[0m                         \n",
       "^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unexpected EOF while parsing\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-3197560269081128>:5\u001B[0;36m\u001B[0m\n\u001B[0;31m    .show(truncate=False)\u001B[0m\n\u001B[0m                         \n^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unexpected EOF while parsing\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: unexpected EOF while parsing (<command-3197560269081128>, line 5)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "\t.option(\"mode\",\"PERMISSIVE\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/single_file_json_with_extra_fields.json\"\\\n",
    "\t.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a468cfa-1b27-4b3c-a2bc-3c95c8404ea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n|age|name  |salary|\n+---+------+------+\n|20 |Manish|20000 |\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "\t.option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .option(\"multiline\",\"true\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/Multi_line_incorrect.json\")\\\n",
    "\t.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab87ff4-e48f-4fac-8021-e9d9c3db89db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+------+\n|     _corrupt_record| age|    name|salary|\n+--------------------+----+--------+------+\n|                null|  20|  Manish| 20000|\n|                   \u0000|null|    null|  null|\n|                null|  25|  Nikita| 21000|\n|                   \u0000|null|    null|  null|\n|                null|  16|  Pritam| 22000|\n|                   \u0000|null|    null|  null|\n|                null|  35|Prantosh| 25000|\n|                   \u0000|null|    null|  null|\n|\u0000{\u0000\"\u0000n\u0000a\u0000m\u0000e\u0000\"\u0000:\u0000...|null|    null|  null|\n+--------------------+----+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "\t.option(\"mode\",\"PERMISSIVE\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/corrupted_json.json\")\\\n",
    "\t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff7926c-e55f-4346-abec-100c886c430b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/spark/pratical/10000Records.csv</td><td>10000Records.csv</td><td>2775309</td><td>1713163917000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/Country_Code.xlsx</td><td>Country_Code.xlsx</td><td>8783</td><td>1712771647000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/ETH_1H.csv</td><td>ETH_1H.csv</td><td>2675166</td><td>1713163920000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/ETH_USD.csv</td><td>ETH_USD.csv</td><td>30306</td><td>1713163919000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/Multi_line_correct.json</td><td>Multi_line_correct.json</td><td>622</td><td>1712769816000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/Multi_line_incorrect.json</td><td>Multi_line_incorrect.json</td><td>1278</td><td>1712769816000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/aadharpancarddata.csv</td><td>aadharpancarddata.csv</td><td>3276</td><td>1713163907000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/bank_full.csv</td><td>bank_full.csv</td><td>4610348</td><td>1713163918000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/corrupted_json.json</td><td>corrupted_json.json</td><td>438</td><td>1712769816000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/date_differ_timezone.csv</td><td>date_differ_timezone.csv</td><td>5424</td><td>1713163917000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/datemultiformat.csv</td><td>datemultiformat.csv</td><td>691</td><td>1713163918000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/emp_details.csv</td><td>emp_details.csv</td><td>686</td><td>1712828417000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file1.json</td><td>file1.json</td><td>2198732</td><td>1712771658000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file2.json</td><td>file2.json</td><td>20006072</td><td>1712771670000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file3.json</td><td>file3.json</td><td>15610388</td><td>1712771670000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file4.json</td><td>file4.json</td><td>16427749</td><td>1712771684000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/file5.json</td><td>file5.json</td><td>669503</td><td>1712771672000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/flipkartdelivary.csv</td><td>flipkartdelivary.csv</td><td>1648</td><td>1713163919000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/healthdata.csv</td><td>healthdata.csv</td><td>343</td><td>1713163920000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/line_delimited_json.json</td><td>line_delimited_json.json</td><td>440</td><td>1712770674000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet</td><td>part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet</td><td>3921</td><td>1712821856000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/single_file_json_with_extra_fields.json</td><td>single_file_json_with_extra_fields.json</td><td>470</td><td>1712769816000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/tourist_Journey_data.csv</td><td>tourist_Journey_data.csv</td><td>7072</td><td>1713163920000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/us_500.csv</td><td>us_500.csv</td><td>92320</td><td>1713163920000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/world_bank.json</td><td>world_bank.json</td><td>2969043</td><td>1713163922000</td></tr><tr><td>dbfs:/FileStore/spark/pratical/zomato.csv</td><td>zomato.csv</td><td>2257316</td><td>1712771676000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/spark/pratical/10000Records.csv",
         "10000Records.csv",
         2775309,
         1713163917000
        ],
        [
         "dbfs:/FileStore/spark/pratical/Country_Code.xlsx",
         "Country_Code.xlsx",
         8783,
         1712771647000
        ],
        [
         "dbfs:/FileStore/spark/pratical/ETH_1H.csv",
         "ETH_1H.csv",
         2675166,
         1713163920000
        ],
        [
         "dbfs:/FileStore/spark/pratical/ETH_USD.csv",
         "ETH_USD.csv",
         30306,
         1713163919000
        ],
        [
         "dbfs:/FileStore/spark/pratical/Multi_line_correct.json",
         "Multi_line_correct.json",
         622,
         1712769816000
        ],
        [
         "dbfs:/FileStore/spark/pratical/Multi_line_incorrect.json",
         "Multi_line_incorrect.json",
         1278,
         1712769816000
        ],
        [
         "dbfs:/FileStore/spark/pratical/aadharpancarddata.csv",
         "aadharpancarddata.csv",
         3276,
         1713163907000
        ],
        [
         "dbfs:/FileStore/spark/pratical/bank_full.csv",
         "bank_full.csv",
         4610348,
         1713163918000
        ],
        [
         "dbfs:/FileStore/spark/pratical/corrupted_json.json",
         "corrupted_json.json",
         438,
         1712769816000
        ],
        [
         "dbfs:/FileStore/spark/pratical/date_differ_timezone.csv",
         "date_differ_timezone.csv",
         5424,
         1713163917000
        ],
        [
         "dbfs:/FileStore/spark/pratical/datemultiformat.csv",
         "datemultiformat.csv",
         691,
         1713163918000
        ],
        [
         "dbfs:/FileStore/spark/pratical/emp_details.csv",
         "emp_details.csv",
         686,
         1712828417000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file1.json",
         "file1.json",
         2198732,
         1712771658000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file2.json",
         "file2.json",
         20006072,
         1712771670000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file3.json",
         "file3.json",
         15610388,
         1712771670000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file4.json",
         "file4.json",
         16427749,
         1712771684000
        ],
        [
         "dbfs:/FileStore/spark/pratical/file5.json",
         "file5.json",
         669503,
         1712771672000
        ],
        [
         "dbfs:/FileStore/spark/pratical/flipkartdelivary.csv",
         "flipkartdelivary.csv",
         1648,
         1713163919000
        ],
        [
         "dbfs:/FileStore/spark/pratical/healthdata.csv",
         "healthdata.csv",
         343,
         1713163920000
        ],
        [
         "dbfs:/FileStore/spark/pratical/line_delimited_json.json",
         "line_delimited_json.json",
         440,
         1712770674000
        ],
        [
         "dbfs:/FileStore/spark/pratical/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet",
         "part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet",
         3921,
         1712821856000
        ],
        [
         "dbfs:/FileStore/spark/pratical/single_file_json_with_extra_fields.json",
         "single_file_json_with_extra_fields.json",
         470,
         1712769816000
        ],
        [
         "dbfs:/FileStore/spark/pratical/tourist_Journey_data.csv",
         "tourist_Journey_data.csv",
         7072,
         1713163920000
        ],
        [
         "dbfs:/FileStore/spark/pratical/us_500.csv",
         "us_500.csv",
         92320,
         1713163920000
        ],
        [
         "dbfs:/FileStore/spark/pratical/world_bank.json",
         "world_bank.json",
         2969043,
         1713163922000
        ],
        [
         "dbfs:/FileStore/spark/pratical/zomato.csv",
         "zomato.csv",
         2257316,
         1712771676000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/spark/pratical/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c6b1b70-8f40-4c9b-8c18-ed9678a4986d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- restaurants: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- restaurant: struct (nullable = true)\n |    |    |    |-- R: struct (nullable = true)\n |    |    |    |    |-- res_id: long (nullable = true)\n |    |    |    |-- apikey: string (nullable = true)\n |    |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |    |-- cuisines: string (nullable = true)\n |    |    |    |-- currency: string (nullable = true)\n |    |    |    |-- deeplink: string (nullable = true)\n |    |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- featured_image: string (nullable = true)\n |    |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |    |-- has_table_booking: long (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |    |-- location: struct (nullable = true)\n |    |    |    |    |-- address: string (nullable = true)\n |    |    |    |    |-- city: string (nullable = true)\n |    |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |    |-- locality: string (nullable = true)\n |    |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |    |-- zipcode: string (nullable = true)\n |    |    |    |-- menu_url: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- offers: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- photos_url: string (nullable = true)\n |    |    |    |-- price_range: long (nullable = true)\n |    |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |    |-- thumb: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |    |-- votes: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "\t.option(\"mode\",\"PERMISSIVE\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/file5.json\")\\\n",
    "\t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af22bb2a-4148-477d-b015-973d6d90449e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n|       United States|             Russia|  156|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/FileStore/spark/pratical/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3339507f-f1ef-4417-a846-9ff2a3283ba7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<div class=\"ansiout\">&lt;console&gt;:5: error: ')' expected but '.' found.\n       dbutils.fs.head(/FileStore/spark/pratical/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet)() // SAFE COMMAND FROM MACRO\n                                                                                                     ^\n</div>",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs \n",
    "head(/FileStore/spark/pratical/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60948ac4-7c80-4745-a400-9b0fe7728901",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3928136799334599>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjson\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minferSchema\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/FileStore/spark/pratical/emp_details.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m    917\u001B[0m     )\n",
       "\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n",
       "\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\n",
       "referenced columns only include the internal corrupt record column\n",
       "(named _corrupt_record by default). For example:\n",
       "spark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\n",
       "and spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\n",
       "Instead, you can cache or save the parsed results and then send the same query.\n",
       "For example, val df = spark.read.schema(schema).csv(file).cache() and then\n",
       "df.filter($\"_corrupt_record\".isNotNull).count()."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3928136799334599>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjson\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minferSchema\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/FileStore/spark/pratical/emp_details.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    917\u001B[0m     )\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/emp_details.csv\").show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbc801f-656d-4e88-9dc4-4f3c216bad34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------------+----------+--------------------+-------+\n|         Name| Age|AadharCardNumber|   PANCard|               Email| Salary|\n+-------------+----+----------------+----------+--------------------+-------+\n|   Ravi Kumar|34.0|  5218 4556 7865|ANXPK2905P|ravi.kumar@gmail.com|65000.0|\n| Kiran Sharma|26.0|  6792 0987 6543|BYDPL6721T|kiran.sharma@yaho...|45000.0|\n| Sanjay Patel|45.0|  8456 3214 9876|AZVPP2987R|sanjay.patel@hotm...|85000.0|\n|  Anita Singh|31.0|  7532 8416 2739|DKPAS1234E|anita.singh@gmail...|55000.0|\n| Vikram Sethi|29.0|  4583 2963 7102|BEDJS7502T|vikram.sethi@hotm...|50000.0|\n|  Priya Mehta|27.0|  5624 3098 6710|AHBPM8435L|priya.mehta@yahoo...|48000.0|\n| Rajesh Verma|42.0|  7631 2406 9028|CYPKV1263F|rajesh.verma@gmai...|75000.0|\n|   Amit Kumar|35.0|  6130 6795 2785|DVKSS0896H|amit.kumar@hotmai...|68000.0|\n| Pooja Sharma|28.0|  1985 7610 2431|AGKPS7521F|pooja.sharma@gmai...|52000.0|\n|   Neha Gupta|33.0|  2841 0376 5619|ECQPL6742T|neha.gupta@yahoo.com|62000.0|\n|Sandeep Singh|37.0|  6154 2897 0734|BSXPD4261R|sandeep.singh@hot...|72000.0|\n|Kavita Sharma|29.0|  8734 0695 4321|CYQDS2742F|kavita.sharma@gma...|50000.0|\n|  Mohit Gupta|31.0|  7369 1540 8267|DITPL3418F|mohit.gupta@hotma...|55000.0|\n|  Nisha Patel|24.0|  9325 7630 6810|AZLPD9021H|nisha.patel@yahoo...|42000.0|\n|Deepak Sharma|44.0|  5472 8935 1201|DSOPS4702Q|deepak.sharma@gma...|80000.0|\n|  Aarti Singh|36.0|  6140 2739 4186|CYVPA7865G|aarti.singh@hotma...|69000.0|\n|  Vikas Verma|39.0|  3706 8429 1603|EBWPD1968R|vikas.verma@gmail...|68000.0|\n| Sonali Patel|25.0|  5690 3018 0246|AJHPM5268P|sonali.patel@yaho...|45000.0|\n| Aakash Singh|30.0|  7823 9460 2756|CYNPA4973G|aakash.singh@hotm...|58000.0|\n|  Nidhi Verma|32.0|  5180 7409 2873|CYGPN9435L|nidhi.verma@gmail...|59000.0|\n+-------------+----+----------------+----------+--------------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.format(\"csv\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "\t.option(\"mode\",\"PERMISSIVE\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/aadharpancarddata.csv\")\\\n",
    "\t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf214c9e-8971-4ecd-8d22-7e7e0c6f8b80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba57b29-892b-44ce-ba79-9f3da0acd775",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2414993855375388>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/spark/practical/pan\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSalary\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-2414993855375388>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/spark/practical/pan\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSalary\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'write'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"overwrite\")\\\n",
    "    .option(\"path\", \"/FileStore/spark/practical/pan\")\\\n",
    "    .partitionBy(\"Salary\")\\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1ec0cb-bcee-45d2-bc74-f20c08fad9e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Name\n",
       "Alice\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Name\nAlice\n\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs head\n",
    "dbfs:/FileStore/spark/practical/pan_card/Salary=10000/part-00002-tid-3923892475674107919-1c6c6ddd-4848-40ef-8cfd-74807d528299-5-1.c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdea6b6-cd2d-4075-aa8d-3a8e2de6ab91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "my_data = [(1,   1),\n",
    "(2,   1),\n",
    "(3,   1),\n",
    "(4,   2),\n",
    "(5,   1),\n",
    "(6,   2),\n",
    "(7,   2)]\n",
    "\n",
    "\n",
    "my_schema = [\"id\", \"num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed409c76-0c4a-45c7-9f9a-7bab01528404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n| id|num|\n+---+---+\n|  1|  1|\n|  2|  1|\n|  3|  1|\n|  4|  2|\n|  5|  1|\n|  6|  2|\n|  7|  2|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "my_df = spark.createDataFrame(data=my_data,schema=my_schema)\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29c93e4-8e20-40e7-8102-730637beb109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Emp ID: integer (nullable = true)\n |-- Name Prefix: string (nullable = true)\n |-- First Name: string (nullable = true)\n |-- Middle Initial: string (nullable = true)\n |-- Last Name: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- E Mail: string (nullable = true)\n |-- Father's Name: string (nullable = true)\n |-- Mother's Name: string (nullable = true)\n |-- Mother's Maiden Name: string (nullable = true)\n |-- Date of Birth: date (nullable = true)\n |-- Time of Birth: string (nullable = true)\n |-- Age in Yrs.: double (nullable = true)\n |-- Weight in Kgs.: integer (nullable = true)\n |-- Date of Joining: date (nullable = true)\n |-- Quarter of Joining: string (nullable = true)\n |-- Half of Joining: string (nullable = true)\n |-- Year of Joining: integer (nullable = true)\n |-- Month of Joining: integer (nullable = true)\n |-- Month Name of Joining: string (nullable = true)\n |-- Short Month: string (nullable = true)\n |-- Day of Joining: integer (nullable = true)\n |-- DOW of Joining: string (nullable = true)\n |-- Short DOW: string (nullable = true)\n |-- Age in Company (Years): double (nullable = true)\n |-- Salary: integer (nullable = true)\n |-- Last % Hike: string (nullable = true)\n |-- SSN: string (nullable = true)\n |-- Phone No. : string (nullable = true)\n |-- Place Name: string (nullable = true)\n |-- County: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Zip: integer (nullable = true)\n |-- Region: string (nullable = true)\n |-- User Name: string (nullable = true)\n |-- Password: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "records_df = spark.read.format(\"csv\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "\t.option(\"mode\",\"PERMISSIVE\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/10000Records.csv\")\\\n",
    "\t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17999371-3f76-4990-a661-e204c9fb4012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rows = (1,\"manish\",26,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edf6e2e-3be6-4033-86fc-603e6f2a1bdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb4195e-ad8d-4669-bdec-d40d31831606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1711827664930549>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mrecords_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m()\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'show'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1711827664930549>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrecords_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m()\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'show'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'show'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "records_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36f4474-82e3-45f5-ac96-723d9585ca2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1711827664930551>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mmy_df\u001B[49m\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'my_df' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1711827664930551>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmy_df\u001B[49m\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'my_df' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'my_df' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_df.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48deb4a-944e-434a-bb68-8ab4c8ecf435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "my_df.select(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ce6749-b03c-4dbb-8b08-4a50f129f0cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|\n+------+--------------+-----------+-------+----------+-------+\n|     1|      John Doe|Engineering| 5000.0|2022-01-01|     30|\n|     2|    Jane Smith|         HR| 6000.0|2022-02-01|     35|\n|     3| Alice Johnson|  Marketing| 5500.0|2022-03-01|     32|\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|\n|     5| Charlie Davis|    Finance| 6500.0|2022-05-01|     40|\n|     6|  David Wilson|      Sales| 5800.0|2022-06-01|     37|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|\n|     8|  Frank Harris|    Finance| 6700.0|2022-08-01|     42|\n|     9|Grace Martinez|         HR| 6200.0|2022-09-01|     33|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|\n+------+--------------+-----------+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Employee DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"emp_dept\", StringType(), True),\n",
    "    StructField(\"emp_sal\", FloatType(), True),\n",
    "    StructField(\"emp_doj\", DateType(), True),\n",
    "    StructField(\"emp_age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 5000.0, datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\").date(), 30),\n",
    "    (2, \"Jane Smith\", \"HR\", 6000.0, datetime.strptime(\"2022-02-01\", \"%Y-%m-%d\").date(), 35),\n",
    "    (3, \"Alice Johnson\", \"Marketing\", 5500.0, datetime.strptime(\"2022-03-01\", \"%Y-%m-%d\").date(), 32),\n",
    "    (4, \"Bob Brown\", \"Engineering\", 5200.0, datetime.strptime(\"2022-04-01\", \"%Y-%m-%d\").date(), 28),\n",
    "    (5, \"Charlie Davis\", \"Finance\", 6500.0, datetime.strptime(\"2022-05-01\", \"%Y-%m-%d\").date(), 40),\n",
    "    (6, \"David Wilson\", \"Sales\", 5800.0, datetime.strptime(\"2022-06-01\", \"%Y-%m-%d\").date(), 37),\n",
    "    (7, \"Emma Lee\", \"Engineering\", 5100.0, datetime.strptime(\"2022-07-01\", \"%Y-%m-%d\").date(), 31),\n",
    "    (8, \"Frank Harris\", \"Finance\", 6700.0, datetime.strptime(\"2022-08-01\", \"%Y-%m-%d\").date(), 42),\n",
    "    (9, \"Grace Martinez\", \"HR\", 6200.0, datetime.strptime(\"2022-09-01\", \"%Y-%m-%d\").date(), 33),\n",
    "    (10, \"Henry Thompson\", \"Marketing\", 5400.0, datetime.strptime(\"2022-10-01\", \"%Y-%m-%d\").date(), 29)\n",
    "]\n",
    "\n",
    "# Create RDD from sample data\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Create DataFrame\n",
    "employ_df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "employ_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383152a0-ab5d-41ad-bd14-921debf0540e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|      emp_name|\n+--------------+\n|      John Doe|\n|    Jane Smith|\n| Alice Johnson|\n|     Bob Brown|\n| Charlie Davis|\n|  David Wilson|\n|      Emma Lee|\n|  Frank Harris|\n|Grace Martinez|\n|Henry Thompson|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.select(\"emp_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19409034-c892-40d2-8d43-c9a835607b27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1711827664930555>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43memploy_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid +5 \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id +5 ` cannot be resolved. Did you mean one of the following? [`emp_age`, `emp_dept`, `emp_doj`, `emp_id`, `emp_name`].;\n",
       "'Project ['id +5 ]\n",
       "+- LogicalRDD [emp_id#2, emp_name#3, emp_dept#4, emp_sal#5, emp_doj#6, emp_age#7], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1711827664930555>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43memploy_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid +5 \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id +5 ` cannot be resolved. Did you mean one of the following? [`emp_age`, `emp_dept`, `emp_doj`, `emp_id`, `emp_name`].;\n'Project ['id +5 ]\n+- LogicalRDD [emp_id#2, emp_name#3, emp_dept#4, emp_sal#5, emp_doj#6, emp_age#7], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id +5 ` cannot be resolved. Did you mean one of the following? [`emp_age`, `emp_dept`, `emp_doj`, `emp_id`, `emp_name`].;\n'Project ['id +5 ]\n+- LogicalRDD [emp_id#2, emp_name#3, emp_dept#4, emp_sal#5, emp_doj#6, emp_age#7], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employ_df.select(\"id +5 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f814b2ad-5c83-4e7b-b530-60425560cd60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|(emp_id + 5)|\n+------------+\n|           6|\n|           7|\n|           8|\n|           9|\n|          10|\n|          11|\n|          12|\n|          13|\n|          14|\n|          15|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.select(col(\"emp_id\") +5 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "408f3d0e-d485-453d-8825-f4e530fd7817",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------+\n|emp_id|      emp_name|emp_age|\n+------+--------------+-------+\n|     1|      John Doe|     30|\n|     2|    Jane Smith|     35|\n|     3| Alice Johnson|     32|\n|     4|     Bob Brown|     28|\n|     5| Charlie Davis|     40|\n|     6|  David Wilson|     37|\n|     7|      Emma Lee|     31|\n|     8|  Frank Harris|     42|\n|     9|Grace Martinez|     33|\n|    10|Henry Thompson|     29|\n+------+--------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.select(col(\"emp_id\"),col(\"emp_name\"),col(\"emp_age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "825839ee-2ba3-4cab-bbfd-f0fb344c0c93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------+-----------+\n|emp_id|      emp_name|emp_sal|   emp_dept|\n+------+--------------+-------+-----------+\n|     1|      John Doe| 5000.0|Engineering|\n|     2|    Jane Smith| 6000.0|         HR|\n|     3| Alice Johnson| 5500.0|  Marketing|\n|     4|     Bob Brown| 5200.0|Engineering|\n|     5| Charlie Davis| 6500.0|    Finance|\n|     6|  David Wilson| 5800.0|      Sales|\n|     7|      Emma Lee| 5100.0|Engineering|\n|     8|  Frank Harris| 6700.0|    Finance|\n|     9|Grace Martinez| 6200.0|         HR|\n|    10|Henry Thompson| 5400.0|  Marketing|\n+------+--------------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.select(\"emp_id\",col(\"emp_name\"),\\\n",
    "    employ_df[\"emp_sal\"],employ_df.emp_dept).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e217d02-f56e-46dd-8216-687833820e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: <bound method DataFrame.show of DataFrame[(emp_id + 5): int]>"
     ]
    }
   ],
   "source": [
    "employ_df.select(expr(\"emp_id +5 \")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bf6d84-093d-4850-9bb3-45c2f851681a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------------+\n|employ_id|   employ_name|concat(emp_name, emp_dept)|\n+---------+--------------+--------------------------+\n|        1|      John Doe|       John DoeEngineering|\n|        2|    Jane Smith|              Jane SmithHR|\n|        3| Alice Johnson|      Alice JohnsonMark...|\n|        4|     Bob Brown|      Bob BrownEngineering|\n|        5| Charlie Davis|      Charlie DavisFinance|\n|        6|  David Wilson|         David WilsonSales|\n|        7|      Emma Lee|       Emma LeeEngineering|\n|        8|  Frank Harris|       Frank HarrisFinance|\n|        9|Grace Martinez|          Grace MartinezHR|\n|       10|Henry Thompson|      Henry ThompsonMar...|\n+---------+--------------+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.select(expr(\"emp_id as employ_id\"),expr(\"emp_name as employ_name\"),expr(\"concat(emp_name,emp_dept)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b2e665-ea4b-46e9-b9e6-f15cb1d1e8db",
     "showTitle": true,
     "title": "Spark SQL "
    }
   },
   "outputs": [],
   "source": [
    "employ_df.createOrReplaceTempView(\"emp_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f8c8627-df5d-4d7d-b5c1-1a8badcd3d8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|\n+------+--------------+-----------+-------+----------+-------+\n|     1|      John Doe|Engineering| 5000.0|2022-01-01|     30|\n|     2|    Jane Smith|         HR| 6000.0|2022-02-01|     35|\n|     3| Alice Johnson|  Marketing| 5500.0|2022-03-01|     32|\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|\n|     5| Charlie Davis|    Finance| 6500.0|2022-05-01|     40|\n|     6|  David Wilson|      Sales| 5800.0|2022-06-01|     37|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|\n|     8|  Frank Harris|    Finance| 6700.0|2022-08-01|     42|\n|     9|Grace Martinez|         HR| 6200.0|2022-09-01|     33|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|\n+------+--------------+-----------+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "          select * from emp_table\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b1edfd-dee7-48e2-93c4-12eee6da18dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|\n+------+--------------+-----------+-------+----------+-------+\n|     1|      John Doe|Engineering| 5000.0|2022-01-01|     30|\n|     2|    Jane Smith|         HR| 6000.0|2022-02-01|     35|\n|     3| Alice Johnson|  Marketing| 5500.0|2022-03-01|     32|\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|\n|     5| Charlie Davis|    Finance| 6500.0|2022-05-01|     40|\n|     6|  David Wilson|      Sales| 5800.0|2022-06-01|     37|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|\n|     8|  Frank Harris|    Finance| 6700.0|2022-08-01|     42|\n|     9|Grace Martinez|         HR| 6200.0|2022-09-01|     33|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|\n+------+--------------+-----------+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b8643f-daa4-4f41-a59d-0415a2f4dd37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n|emp_id|emp_age|\n+------+-------+\n|     1|     30|\n|     2|     35|\n|     3|     32|\n|     4|     28|\n|     5|     40|\n|     6|     37|\n|     7|     31|\n|     8|     42|\n|     9|     33|\n|    10|     29|\n+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select emp_id,emp_age from emp_table \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ccb98d-5c90-4b9b-8c2c-4520296f2dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: integer (nullable = true)\n |-- emp_name: string (nullable = true)\n |-- emp_dept: string (nullable = true)\n |-- emp_sal: float (nullable = true)\n |-- emp_doj: date (nullable = true)\n |-- emp_age: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5caf7200-afe3-4819-b7be-67fab5dc9897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------+\n|employee_id|      emp_name|emp_age|\n+-----------+--------------+-------+\n|          1|      John Doe|     30|\n|          2|    Jane Smith|     35|\n|          3| Alice Johnson|     32|\n|          4|     Bob Brown|     28|\n|          5| Charlie Davis|     40|\n|          6|  David Wilson|     37|\n|          7|      Emma Lee|     31|\n|          8|  Frank Harris|     42|\n|          9|Grace Martinez|     33|\n|         10|Henry Thompson|     29|\n+-----------+--------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.select(col(\"emp_id\").alias(\"employee_id\"),\"emp_name\",\"emp_age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88ed0b5-6c3b-4f2d-a19c-e854531bdf3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|\n+------+--------------+-----------+-------+----------+-------+\n|     2|    Jane Smith|         HR| 6000.0|2022-02-01|     35|\n|     3| Alice Johnson|  Marketing| 5500.0|2022-03-01|     32|\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|\n|     5| Charlie Davis|    Finance| 6500.0|2022-05-01|     40|\n|     6|  David Wilson|      Sales| 5800.0|2022-06-01|     37|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|\n|     8|  Frank Harris|    Finance| 6700.0|2022-08-01|     42|\n|     9|Grace Martinez|         HR| 6200.0|2022-09-01|     33|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|\n+------+--------------+-----------+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.filter(col(\"emp_sal\")>5000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c3c102-c2da-4942-984d-e8cf87e87a09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|\n+------+--------------+-----------+-------+----------+-------+\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|\n+------+--------------+-----------+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.filter((col(\"emp_sal\")>5000) & (col(\"emp_age\")<32)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba6f8da-a70e-4af3-9150-ba79041ddd94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|\n+------+--------------+-----------+-------+----------+-------+\n|     2|    Jane Smith|         HR| 6000.0|2022-02-01|     35|\n|     3| Alice Johnson|  Marketing| 5500.0|2022-03-01|     32|\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|\n|     5| Charlie Davis|    Finance| 6500.0|2022-05-01|     40|\n|     6|  David Wilson|      Sales| 5800.0|2022-06-01|     37|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|\n|     8|  Frank Harris|    Finance| 6700.0|2022-08-01|     42|\n|     9|Grace Martinez|         HR| 6200.0|2022-09-01|     33|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|\n+------+--------------+-----------+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.where(col(\"emp_sal\")>5000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a6bcc4-e428-4077-8f29-9e3101f320cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+-----------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|middle_name|\n+------+--------------+-----------+-------+----------+-------+-----------+\n|     1|      John Doe|Engineering| 5000.0|2022-01-01|     30|      kumar|\n|     2|    Jane Smith|         HR| 6000.0|2022-02-01|     35|      kumar|\n|     3| Alice Johnson|  Marketing| 5500.0|2022-03-01|     32|      kumar|\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|      kumar|\n|     5| Charlie Davis|    Finance| 6500.0|2022-05-01|     40|      kumar|\n|     6|  David Wilson|      Sales| 5800.0|2022-06-01|     37|      kumar|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|      kumar|\n|     8|  Frank Harris|    Finance| 6700.0|2022-08-01|     42|      kumar|\n|     9|Grace Martinez|         HR| 6200.0|2022-09-01|     33|      kumar|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|      kumar|\n+------+--------------+-----------+-------+----------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.select(\"*\",lit(\"kumar\").alias(\"middle_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af7987a-bc10-41f1-8417-ac7bfec3f115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+--------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|sur_name|\n+------+--------------+-----------+-------+----------+-------+--------+\n|     1|      John Doe|Engineering| 5000.0|2022-01-01|     30|   singh|\n|     2|    Jane Smith|         HR| 6000.0|2022-02-01|     35|   singh|\n|     3| Alice Johnson|  Marketing| 5500.0|2022-03-01|     32|   singh|\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|   singh|\n|     5| Charlie Davis|    Finance| 6500.0|2022-05-01|     40|   singh|\n|     6|  David Wilson|      Sales| 5800.0|2022-06-01|     37|   singh|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|   singh|\n|     8|  Frank Harris|    Finance| 6700.0|2022-08-01|     42|   singh|\n|     9|Grace Martinez|         HR| 6200.0|2022-09-01|     33|   singh|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|   singh|\n+------+--------------+-----------+-------+----------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.withColumn(\"sur_name\",lit(\"singh\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6c88b4-be90-42d3-87a3-a7da9204ee03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+-------+----------+-------+\n|employ_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|\n+---------+--------------+-----------+-------+----------+-------+\n|        1|      John Doe|Engineering| 5000.0|2022-01-01|     30|\n|        2|    Jane Smith|         HR| 6000.0|2022-02-01|     35|\n|        3| Alice Johnson|  Marketing| 5500.0|2022-03-01|     32|\n|        4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|\n|        5| Charlie Davis|    Finance| 6500.0|2022-05-01|     40|\n|        6|  David Wilson|      Sales| 5800.0|2022-06-01|     37|\n|        7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|\n|        8|  Frank Harris|    Finance| 6700.0|2022-08-01|     42|\n|        9|Grace Martinez|         HR| 6200.0|2022-09-01|     33|\n|       10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|\n+---------+--------------+-----------+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.withColumnRenamed(\"emp_id\",\"employ_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b2b782-bda9-4f48-93a3-a6590ea2c572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: integer (nullable = true)\n |-- emp_name: string (nullable = true)\n |-- emp_dept: string (nullable = true)\n |-- emp_sal: float (nullable = true)\n |-- emp_doj: date (nullable = true)\n |-- emp_age: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb7d121-315e-4ed7-b694-e845ca559262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: string (nullable = true)\n |-- emp_name: string (nullable = true)\n |-- emp_dept: string (nullable = true)\n |-- emp_sal: float (nullable = true)\n |-- emp_doj: date (nullable = true)\n |-- emp_age: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.withColumn(\"emp_id\",col(\"emp_id\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81ab4de-cad5-4824-8d34-fef6d2e8f672",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: string (nullable = true)\n |-- emp_name: string (nullable = true)\n |-- emp_dept: string (nullable = true)\n |-- emp_sal: long (nullable = true)\n |-- emp_doj: date (nullable = true)\n |-- emp_age: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.withColumn(\"emp_id\",col(\"emp_id\").cast(\"string\"))\\\n",
    "    .withColumn(\"emp_sal\",col(\"emp_sal\").cast(\"long\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b98d98-6999-4f93-accf-72a03055c229",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+\n|emp_id|      emp_name|   emp_dept|emp_sal|\n+------+--------------+-----------+-------+\n|     1|      John Doe|Engineering| 5000.0|\n|     2|    Jane Smith|         HR| 6000.0|\n|     3| Alice Johnson|  Marketing| 5500.0|\n|     4|     Bob Brown|Engineering| 5200.0|\n|     5| Charlie Davis|    Finance| 6500.0|\n|     6|  David Wilson|      Sales| 5800.0|\n|     7|      Emma Lee|Engineering| 5100.0|\n|     8|  Frank Harris|    Finance| 6700.0|\n|     9|Grace Martinez|         HR| 6200.0|\n|    10|Henry Thompson|  Marketing| 5400.0|\n+------+--------------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employ_df.drop(\"emp_age\",col(\"emp_doj\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03b7709-3532-4fb5-8d32-bf8978a8bb0a",
     "showTitle": true,
     "title": "Spark SQL "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|\n+------+--------------+-----------+-------+----------+-------+\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|\n+------+--------------+-----------+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from emp_table where emp_sal > 5000 and \n",
    "          emp_age < 32 \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5d7552-0513-493d-a961-4ecc944a1dff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+-------+----------+-------+---------+\n|emp_id|      emp_name|   emp_dept|emp_sal|   emp_doj|emp_age|last_name|\n+------+--------------+-----------+-------+----------+-------+---------+\n|     4|     Bob Brown|Engineering| 5200.0|2022-04-01|     28|    kumar|\n|     7|      Emma Lee|Engineering| 5100.0|2022-07-01|     31|    kumar|\n|    10|Henry Thompson|  Marketing| 5400.0|2022-10-01|     29|    kumar|\n+------+--------------+-----------+-------+----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select *, \"kumar\" as last_name from emp_table where emp_sal > 5000 and \n",
    "          emp_age < 32 \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b6abc71-227a-49b8-8521-3831796411bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a36b03-2e39-4427-8f79-04b12911eacd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = ['id','name','sal','mangr_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5fa8c8-d4a6-4cc4-bfaf-3d214c5ea1aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "manager_df = spark.createDataFrame(data=my_data,schema=my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f138e3-035a-4b03-ad57-d65db26c09b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17)]\n",
    "\n",
    "schema_1 = ['id','name','sal','mangr_id']\n",
    "\n",
    "manger_df1 = spark.createDataFrame(data=data1,schema=schema_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4adc6e8a-a2d5-4453-b777-42f300684cf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 10|  Anil|50000|      18|\n| 11| Vikas|75000|      16|\n| 12| Nisha|40000|      18|\n| 13| Nidhi|60000|      17|\n| 14| Priya|80000|      18|\n| 15| Mohit|45000|      18|\n| 16|Rajesh|90000|      10|\n| 17| Raman|55000|      16|\n| 18|   Sam|65000|      17|\n+---+------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c76abf5-caf8-4e88-95a6-5f8635a226e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: 9"
     ]
    }
   ],
   "source": [
    "manager_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4890212-9389-47b5-9285-615f800636e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+--------+\n| id| name|  sal|mangr_id|\n+---+-----+-----+--------+\n| 19|Sohan|50000|      18|\n| 20| Sima|75000|      17|\n+---+-----+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a94d383-310d-49c0-aa95-3b74df2eeb97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 10|  Anil|50000|      18|\n| 11| Vikas|75000|      16|\n| 12| Nisha|40000|      18|\n| 13| Nidhi|60000|      17|\n| 14| Priya|80000|      18|\n| 15| Mohit|45000|      18|\n| 16|Rajesh|90000|      10|\n| 17| Raman|55000|      16|\n| 18|   Sam|65000|      17|\n| 19| Sohan|50000|      18|\n| 20|  Sima|75000|      17|\n+---+------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.union(manger_df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32140bc7-56b1-4c0a-a8d5-2a115d1a835d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 10|  Anil|50000|      18|\n| 11| Vikas|75000|      16|\n| 12| Nisha|40000|      18|\n| 13| Nidhi|60000|      17|\n| 14| Priya|80000|      18|\n| 15| Mohit|45000|      18|\n| 16|Rajesh|90000|      10|\n| 17| Raman|55000|      16|\n| 18|   Sam|65000|      17|\n| 19| Sohan|50000|      18|\n| 20|  Sima|75000|      17|\n+---+------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.unionAll(manger_df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b54c43-7923-433f-9e00-61fe500c524e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f58ad19-348e-4f88-8b12-c9be434a0a42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duplicate_rows = [\n",
    "    Row(id=20, name='Sima', sal=75000, mangr_id=17),\n",
    "    Row(id=21, name='Raj', sal=60000, mangr_id=16),\n",
    "    Row(id=22, name='Priya', sal=80000, mangr_id=18)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21960181-e011-4b77-98ed-06d1650f1629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef36518-68f2-480a-a19b-f05c69fe60a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert duplicate_rows to a DataFrame\n",
    "\n",
    "\n",
    "# Append new_df to manager_df\n",
    "combined_df = manager_df.unionAll(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5cace21-efae-472c-9a0c-8de4c9ee9736",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 10|  Anil|50000|      18|\n| 11| Vikas|75000|      16|\n| 12| Nisha|40000|      18|\n| 13| Nidhi|60000|      17|\n| 14| Priya|80000|      18|\n| 15| Mohit|45000|      18|\n| 16|Rajesh|90000|      10|\n| 17| Raman|55000|      16|\n| 18|   Sam|65000|      17|\n| 20|  Sima|75000|      17|\n| 21|   Raj|60000|      16|\n| 22| Priya|80000|      18|\n+---+------+-----+--------+\n\nTotal number of rows after adding duplicates: 12\n"
     ]
    }
   ],
   "source": [
    "# Display the combined DataFrame\n",
    "combined_df.show()\n",
    "\n",
    "# Optionally remove duplicates\n",
    "# combined_df = combined_df.dropDuplicates()\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = combined_df.count()\n",
    "print(\"Total number of rows after adding duplicates:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326640d6-ebd7-4af5-833e-e73a5a5eb4fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[37]: 14"
     ]
    }
   ],
   "source": [
    "combined_df.unionAll(manger_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2addf5ac-6de5-4552-ac45-b794e77004e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: 14"
     ]
    }
   ],
   "source": [
    "combined_df.union(manger_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35baff23-952b-4c28-b295-011974b4c0d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "manger_df1.createOrReplaceTempView(\"manger_df1_tabl\")\n",
    "combined_df.createOrReplaceTempView(\"duplicate_manger_df_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101af246-ecc3-4eb4-86de-ef88247b1eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: 13"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from manger_df1_tabl\n",
    "          union \n",
    "          select * from duplicate_manger_df_tbl \"\"\" ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a3e8e8-897d-4ec4-816a-db2d09d47477",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: 14"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from manger_df1_tabl\n",
    "          union all \n",
    "          select * from duplicate_manger_df_tbl \"\"\" ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe9f4e1-2a78-45af-b47f-aaaf08b18c69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "\n",
    "wrong_schema = ['id','sal','mangr_id','name']\n",
    "\n",
    "wrong_manger_df = spark.createDataFrame(data=wrong_column_data,schema=wrong_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87be68f-0e1b-4e18-8045-7a623ad3ed40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+--------+\n| id| name|  sal|mangr_id|\n+---+-----+-----+--------+\n| 19|Sohan|50000|      18|\n| 20| Sima|75000|      17|\n| 19|50000|   18|   Sohan|\n| 20|75000|   17|    Sima|\n+---+-----+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df1.union(wrong_manger_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd54404e-260b-4637-8c86-7d1eecba5ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+--------+\n| id| name|  sal|mangr_id|\n+---+-----+-----+--------+\n| 19|Sohan|50000|      18|\n| 20| Sima|75000|      17|\n+---+-----+-----+--------+\n\n+---+-----+--------+-----+\n| id|  sal|mangr_id| name|\n+---+-----+--------+-----+\n| 19|50000|      18|Sohan|\n| 20|75000|      17| Sima|\n+---+-----+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df1.show()\n",
    "\n",
    "wrong_manger_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d5994d-a084-4ab2-8aad-be73bfb5fa23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+--------+\n| id| name|  sal|mangr_id|\n+---+-----+-----+--------+\n| 19|Sohan|50000|      18|\n| 20| Sima|75000|      17|\n| 19|Sohan|50000|      18|\n| 20| Sima|75000|      17|\n+---+-----+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df1.unionByName(wrong_manger_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca5d827-60da-4fb7-b347-322e35c8e0b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan',10),\n",
    "(20 ,75000,  17,'Sima',20)]\n",
    "\n",
    "wrong_schema = ['id','sal','mangr_id','name','bonus']\n",
    "\n",
    "wrong_manger_df = spark.createDataFrame(data=wrong_column_data,schema=wrong_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efce0ace-6ffd-4242-86c9-c261aa6d9a72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-133874981705034>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mwrong_manger_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanger_df1\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3646\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n",
       "\u001B[1;32m   3598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3599\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3601\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3644\u001B[0m \u001B[38;5;124;03m    +----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 5 columns and the second input has 4 columns.;\n",
       "'Union false, false\n",
       ":- LogicalRDD [id#767L, sal#768L, mangr_id#769L, name#770, bonus#771L], false\n",
       "+- LogicalRDD [id#144L, name#145, sal#146L, mangr_id#147L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-133874981705034>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mwrong_manger_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanger_df1\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3646\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   3598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3599\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3601\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3644\u001B[0m \u001B[38;5;124;03m    +----+----+----+\u001B[39;00m\n\u001B[1;32m   3645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 5 columns and the second input has 4 columns.;\n'Union false, false\n:- LogicalRDD [id#767L, sal#768L, mangr_id#769L, name#770, bonus#771L], false\n+- LogicalRDD [id#144L, name#145, sal#146L, mangr_id#147L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 5 columns and the second input has 4 columns.;\n'Union false, false\n:- LogicalRDD [id#767L, sal#768L, mangr_id#769L, name#770, bonus#771L], false\n+- LogicalRDD [id#144L, name#145, sal#146L, mangr_id#147L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrong_manger_df.union(manger_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05e9a3d-aeca-42ab-9ccf-5397b62143f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-133874981705035>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mwrong_manger_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msal\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmanger_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39munion(manger_df1)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `manger_id` cannot be resolved. Did you mean one of the following? [`mangr_id`, `id`, `name`, `bonus`, `sal`].;\n",
       "'Project [id#767L, sal#768L, 'manger_id, name#770]\n",
       "+- LogicalRDD [id#767L, sal#768L, mangr_id#769L, name#770, bonus#771L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-133874981705035>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mwrong_manger_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msal\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmanger_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39munion(manger_df1)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `manger_id` cannot be resolved. Did you mean one of the following? [`mangr_id`, `id`, `name`, `bonus`, `sal`].;\n'Project [id#767L, sal#768L, 'manger_id, name#770]\n+- LogicalRDD [id#767L, sal#768L, mangr_id#769L, name#770, bonus#771L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `manger_id` cannot be resolved. Did you mean one of the following? [`mangr_id`, `id`, `name`, `bonus`, `sal`].;\n'Project [id#767L, sal#768L, 'manger_id, name#770]\n+- LogicalRDD [id#767L, sal#768L, mangr_id#769L, name#770, bonus#771L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrong_manger_df.select('id','sal','manger_id','name').union(manger_df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43f6aa6f-dcaa-4afe-b316-bcc6c877cc9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "\n",
    "wrong_schema = ['id','sal','mangr_id','nam']\n",
    "\n",
    "wrong_manger_df2 = spark.createDataFrame(data=wrong_column_data,schema=wrong_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81aff642-f5f6-46af-be7c-2b7098ec28d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-133874981705037>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mwrong_manger_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrong_manger_df2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n",
       "\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3685\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot resolve column name \"name\" among (id, sal, mangr_id, nam)."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-133874981705037>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mwrong_manger_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrong_manger_df2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3685\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot resolve column name \"name\" among (id, sal, mangr_id, nam).",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot resolve column name \"name\" among (id, sal, mangr_id, nam).",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrong_manger_df.unionByName(wrong_manger_df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82cac248-241f-40c2-8e45-7e47997daa9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db31f31-7b35-46cc-aa65-1c040aa00ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_schema = ['id','name','age','salary','country','dept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f95c4a4-a5b4-46ae-a5c7-540965ae4b19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(data=emp_data,schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed44bb7c-b3b4-43cf-9148-4e14aa3c7a54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|salary|country|       dept|\n+----+-------+----+------+-------+-----------+\n|   1| manish|  26| 20000|  india|         IT|\n|   2|  rahul|null| 40000|germany|engineering|\n|   3|  pawan|  12| 60000|  india|      sales|\n|   4|roshini|  44|  null|     uk|engineering|\n|   5|raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   adam|  37| 65000|     us|         IT|\n|   8|  chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac718ffa-0389-4957-aeec-83cc5ae5214b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bdedd87-d7fa-4982-b2dc-2f3cc0cf948b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+-------+\n|  id|   name| age|salary|country|       dept|  adult|\n+----+-------+----+------+-------+-----------+-------+\n|   1| manish|  26| 20000|  india|         IT|    yes|\n|   2|  rahul|null| 40000|germany|engineering|novalue|\n|   3|  pawan|  12| 60000|  india|      sales|     no|\n|   4|roshini|  44|  null|     uk|engineering|    yes|\n|   5|raushan|  35| 70000|  india|      sales|    yes|\n|   6|   null|  29|200000|     uk|         IT|    yes|\n|   7|   adam|  37| 65000|     us|         IT|    yes|\n|   8|  chris|  16| 40000|     us|      sales|     no|\n|null|   null|null|  null|   null|       null|novalue|\n|   7|   adam|  37| 65000|     us|         IT|    yes|\n+----+-------+----+------+-------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"adult\",when(col(\"age\")<18,\"no\")\n",
    "                  .when(col(\"age\")>18,\"yes\")\n",
    "                  .otherwise(\"novalue\")).show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa51639-ced0-4fab-83fb-75893f5c8070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+------+-------+-----------+-----+\n|  id|   name|age|salary|country|       dept|adult|\n+----+-------+---+------+-------+-----------+-----+\n|   1| manish| 26| 20000|  india|         IT|  yes|\n|   2|  rahul| 19| 40000|germany|engineering|  yes|\n|   3|  pawan| 12| 60000|  india|      sales|   no|\n|   4|roshini| 44|  null|     uk|engineering|  yes|\n|   5|raushan| 35| 70000|  india|      sales|  yes|\n|   6|   null| 29|200000|     uk|         IT|  yes|\n|   7|   adam| 37| 65000|     us|         IT|  yes|\n|   8|  chris| 16| 40000|     us|      sales|   no|\n|null|   null| 19|  null|   null|       null|  yes|\n|   7|   adam| 37| 65000|     us|         IT|  yes|\n+----+-------+---+------+-------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"age\",when(col(\"age\").isNull(),lit(19))\n",
    "                  .otherwise(col(\"age\")))\\\n",
    "    .withColumn(\"adult\",when(col(\"age\")>18,\"yes\")\n",
    "                .otherwise(\"no\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1856011d-4d9e-4c52-a27d-08312fa6daa2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n|  id|   name| age|salary|country|       dept|age_wise|\n+----+-------+----+------+-------+-----------+--------+\n|   1| manish|  26| 20000|  india|         IT|    null|\n|   2|  rahul|null| 40000|germany|engineering|    null|\n|   3|  pawan|  12| 60000|  india|      sales|   minor|\n|   4|roshini|  44|  null|     uk|engineering|    null|\n|   5|raushan|  35| 70000|  india|      sales|    null|\n|   6|   null|  29|200000|     uk|         IT|    null|\n|   7|   adam|  37| 65000|     us|         IT|    null|\n|   8|  chris|  16| 40000|     us|      sales|   minor|\n|null|   null|null|  null|   null|       null|    null|\n|   7|   adam|  37| 65000|     us|         IT|    null|\n+----+-------+----+------+-------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"age_wise\",when((col(\"age\")>0) &(col(\"age\")<18),\"minor\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383d5000-2ac1-4dc2-8c40-fe40a88982ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n|  id|   name| age|salary|country|       dept|age_wise|\n+----+-------+----+------+-------+-----------+--------+\n|   1| manish|  26| 20000|  india|         IT|     mid|\n|   2|  rahul|null| 40000|germany|engineering|   major|\n|   3|  pawan|  12| 60000|  india|      sales|   minor|\n|   4|roshini|  44|  null|     uk|engineering|   major|\n|   5|raushan|  35| 70000|  india|      sales|   major|\n|   6|   null|  29|200000|     uk|         IT|     mid|\n|   7|   adam|  37| 65000|     us|         IT|   major|\n|   8|  chris|  16| 40000|     us|      sales|   minor|\n|null|   null|null|  null|   null|       null|   major|\n|   7|   adam|  37| 65000|     us|         IT|   major|\n+----+-------+----+------+-------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"age_wise\",when((col(\"age\")>0) & (col(\"age\")<18),\"minor\")\n",
    "                  .when((col(\"age\")>18) & (col(\"age\")<30),\"mid\")\n",
    "                  .otherwise(\"major\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f14fe5-eea7-43fa-b392-b10064e65b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView(\"tab1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8689e93b-a671-4a45-bd11-514a4a74ad76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n|  id|   name| age|salary|country|       dept|age_wise|\n+----+-------+----+------+-------+-----------+--------+\n|   1| manish|  26| 20000|  india|         IT|   major|\n|   2|  rahul|null| 40000|germany|engineering| novalue|\n|   3|  pawan|  12| 60000|  india|      sales|   minor|\n|   4|roshini|  44|  null|     uk|engineering|   major|\n|   5|raushan|  35| 70000|  india|      sales|   major|\n|   6|   null|  29|200000|     uk|         IT|   major|\n|   7|   adam|  37| 65000|     us|         IT|   major|\n|   8|  chris|  16| 40000|     us|      sales|   minor|\n|null|   null|null|  null|   null|       null| novalue|\n|   7|   adam|  37| 65000|     us|         IT|   major|\n+----+-------+----+------+-------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select *,\n",
    "          case when age< 18 then 'minor'\n",
    "          when age>18 then 'major'\n",
    "          else 'novalue'\n",
    "          end as age_wise\n",
    "          from tab1 \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2145e02e-d55d-4ea4-a228-564603342493",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(13 ,'Nidhi',60000,  17),      \n",
    "(14 ,'Priya',90000,  18),  \n",
    "(18 ,'Sam',65000,   17)\n",
    "     ]\n",
    "\n",
    "my_schema = ['id','name','sal','mangr_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1cd6f0e-4c22-47af-aaf1-6d75c81a9eb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "manger_df = spark.createDataFrame(data=my_data,schema=my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8348c6a8-98fa-45f1-9950-aa60ca37ad3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 10|  Anil|50000|      18|\n| 11| Vikas|75000|      16|\n| 12| Nisha|40000|      18|\n| 13| Nidhi|60000|      17|\n| 14| Priya|80000|      18|\n| 15| Mohit|45000|      18|\n| 16|Rajesh|90000|      10|\n| 17| Raman|55000|      16|\n| 18|   Sam|65000|      17|\n| 15| Mohit|45000|      18|\n| 13| Nidhi|60000|      17|\n| 14| Priya|90000|      18|\n| 18|   Sam|65000|      17|\n+---+------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7d93fc-94cb-432a-a872-b8a4b5aaa26d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 10|  Anil|50000|      18|\n| 12| Nisha|40000|      18|\n| 11| Vikas|75000|      16|\n| 13| Nidhi|60000|      17|\n| 15| Mohit|45000|      18|\n| 14| Priya|80000|      18|\n| 16|Rajesh|90000|      10|\n| 17| Raman|55000|      16|\n| 18|   Sam|65000|      17|\n| 14| Priya|90000|      18|\n+---+------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ce793b-2d15-4211-a306-a8d7c61f5de9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4219618486453525>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mmanger_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistinct\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: distinct() takes 1 positional argument but 3 were given"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4219618486453525>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmanger_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistinct\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\n\u001B[0;31mTypeError\u001B[0m: distinct() takes 1 positional argument but 3 were given",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: distinct() takes 1 positional argument but 3 were given",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "manger_df.distinct(\"id\",\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f742ec32-7cb3-4b61-ac2c-d4700ffce330",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n| 10|  Anil|\n| 11| Vikas|\n| 12| Nisha|\n| 13| Nidhi|\n| 15| Mohit|\n| 14| Priya|\n| 17| Raman|\n| 16|Rajesh|\n| 18|   Sam|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df.select(\"id\",\"name\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54fa99dd-31c7-490c-ad2d-6f5af447644f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 10|  Anil|50000|      18|\n| 11| Vikas|75000|      16|\n| 12| Nisha|40000|      18|\n| 13| Nidhi|60000|      17|\n| 15| Mohit|45000|      18|\n| 14| Priya|80000|      18|\n| 17| Raman|55000|      16|\n| 16|Rajesh|90000|      10|\n| 18|   Sam|65000|      17|\n+---+------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "drop_manger_df = manger_df.drop_duplicates([\"id\",\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317f61d7-aaf6-45b6-9c29-52bb61437b69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 10|  Anil|50000|      18|\n| 11| Vikas|75000|      16|\n| 12| Nisha|40000|      18|\n| 13| Nidhi|60000|      17|\n| 13| Nidhi|60000|      17|\n| 14| Priya|90000|      18|\n| 14| Priya|80000|      18|\n| 15| Mohit|45000|      18|\n| 15| Mohit|45000|      18|\n| 16|Rajesh|90000|      10|\n| 17| Raman|55000|      16|\n| 18|   Sam|65000|      17|\n| 18|   Sam|65000|      17|\n+---+------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df.sort(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0685ee-4a7a-4ae5-b9f2-fe21f99acace",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+\n| id|  name|  sal|mangr_id|\n+---+------+-----+--------+\n| 16|Rajesh|90000|      10|\n| 14| Priya|90000|      18|\n| 14| Priya|80000|      18|\n| 11| Vikas|75000|      16|\n| 18|   Sam|65000|      17|\n| 18|   Sam|65000|      17|\n| 13| Nidhi|60000|      17|\n| 13| Nidhi|60000|      17|\n| 17| Raman|55000|      16|\n| 10|  Anil|50000|      18|\n| 15| Mohit|45000|      18|\n| 15| Mohit|45000|      18|\n| 12| Nisha|40000|      18|\n+---+------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "manger_df.sort(col(\"sal\").desc(),col(\"name\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2cf5155-d294-451b-9715-6fdabbf219d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leet_code_data = [\n",
    "    (1, 'Will', None),\n",
    "    (2, 'Jane', None),\n",
    "    (3, 'Alex', 2),\n",
    "    (4, 'Bill', None),\n",
    "    (5, 'Zack', 1),\n",
    "    (6, 'Mark', 2)\n",
    "]\n",
    "\n",
    "schema_let = ['id','name','refernce_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1190b4e0-bfd9-4266-950d-42132081a11e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leet_df = spark.createDataFrame(data=leet_code_data,schema=schema_let)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fa407a-b5bb-4d40-9132-d613486e9b9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------+\n| id|name|refernce_id|\n+---+----+-----------+\n|  1|Will|       null|\n|  2|Jane|       null|\n|  3|Alex|          2|\n|  4|Bill|       null|\n|  5|Zack|          1|\n|  6|Mark|          2|\n+---+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "leet_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc9e7ac-8816-44b5-a110-4ee445f1c614",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "emp_schema = ['id','name','age','salary','country','dept']\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data,schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8351a808-4644-461f-922c-4503aed2afca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|salary|country|       dept|\n+----+-------+----+------+-------+-----------+\n|   1| manish|  26| 20000|  india|         IT|\n|   2|  rahul|null| 40000|germany|engineering|\n|   3|  pawan|  12| 60000|  india|      sales|\n|   4|roshini|  44|  null|     uk|engineering|\n|   5|raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   adam|  37| 65000|     us|         IT|\n|   8|  chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921d4641-524c-4401-b87a-6e520a42ba53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: 10"
     ]
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b429b5-30a6-4680-bdb2-3707370d9e3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|count(name)|\n+-----------+\n|          8|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c01f4d-b370-4fc7-b80c-409e0e776305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|count(id)|\n+---------+\n|        9|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3048f1cb-bf5a-4bcc-a7bb-2b4373c28df9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 10"
     ]
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2224e57-8179-4072-a094-ae37a03a7331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|count(country)|\n+--------------+\n|             9|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(\"country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be4391a-5ee0-4376-8d80-23d4102ac09a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|      10|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c01773f-5904-403b-9a6a-deb6e73d11e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n|sum(salary)|max(salary)|min(salary)|\n+-----------+-----------+-----------+\n|     560000|     200000|      20000|\n+-----------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(sum(\"salary\"),max(\"salary\"),min(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d88a0f-2b60-4bcb-87c7-369f7b852731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n|tot_sal|max_sal|min_sal|\n+-------+-------+-------+\n| 560000| 200000|  20000|\n+-------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(sum(\"salary\").alias(\"tot_sal\"),max(\"salary\").alias(\"max_sal\"),min(\"salary\").alias(\"min_sal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e07f6e9-caf7-480f-b2a1-3b5a98df790b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+\n|sum(salary)|count(salary)|avg_sal|\n+-----------+-------------+-------+\n|     560000|            8|  70000|\n+-----------+-------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(sum(\"salary\"),count(\"salary\"),avg(\"salary\").cast(\"int\").alias(\"avg_sal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b74707f-5c5a-4ad0-9854-1dfaaf475369",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data= [(1,'manish',50000,'IT','india'),\n",
    "(2,'vikash',60000,'sales','us'),\n",
    "(3,'raushan',70000,'marketing','india'),\n",
    "(4,'mukesh',80000,'IT','us'),\n",
    "(5,'pritam',90000,'sales','india'),\n",
    "(6,'nikita',45000,'marketing','us'),\n",
    "(7,'ragini',55000,'marketing','india'),\n",
    "(8,'rakesh',100000,'IT','us'),\n",
    "(9,'aditya',65000,'IT','india'),\n",
    "(10,'rahul',50000,'marketing','us')]\n",
    "\n",
    "schema = [\"id\",'name','sal','dept','country']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577dd5d4-b59f-4a04-a7a4-ee27e995eaed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(data=data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1d8671-8fd7-4763-ae2f-ddf35791d854",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+-------+\n| id|   name|   sal|     dept|country|\n+---+-------+------+---------+-------+\n|  1| manish| 50000|       IT|  india|\n|  2| vikash| 60000|    sales|     us|\n|  3|raushan| 70000|marketing|  india|\n|  4| mukesh| 80000|       IT|     us|\n|  5| pritam| 90000|    sales|  india|\n|  6| nikita| 45000|marketing|     us|\n|  7| ragini| 55000|marketing|  india|\n|  8| rakesh|100000|       IT|     us|\n|  9| aditya| 65000|       IT|  india|\n| 10|  rahul| 50000|marketing|     us|\n+---+-------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af34c9a8-32fc-4284-825d-7d1ac5675a5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|     dept|sum(sal)|\n+---------+--------+\n|       IT|  295000|\n|    sales|  150000|\n|marketing|  220000|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy(\"dept\")\\\n",
    ".agg(sum(\"sal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a07edb3-93ff-4112-8ec3-fdf622adbe9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView(\"tab1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7420b9b4-30df-4621-98a4-4cc2842ef689",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|     dept|sum(sal)|\n+---------+--------+\n|       IT|    null|\n|    sales|    null|\n|marketing|    null|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          select dept,sum(\"sal\")\n",
    "          from tab1\n",
    "          group by dept \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2371ea1-d58b-4679-8f9b-c6665066e5ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "(2,'vikash','kolkata',\"12-03-2023\"),\n",
    "(3,'nikita','delhi',\"25-06-2023\"),\n",
    "(4,'rahul','ranchi',\"24-03-2023\"),\n",
    "(5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "(6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "(7,'raman','patna',\"30-12-2022\"),\n",
    "(8,'prakash','ranchi',\"24-02-2023\"),\n",
    "(9,'ragini','kolkata',\"03-03-2023\"),\n",
    "(10,'raushan','jaipur',\"05-02-2023\")]\n",
    "\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "\n",
    "\n",
    "customer_df = spark.createDataFrame(data=customer_data,schema=customer_schema)\n",
    "\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "(1,27,5,\"03-02-2023\"),\n",
    "(2,5,3,\"01-06-2023\"),\n",
    "(5,22,1,\"22-03-2023\"),\n",
    "(7,22,4,\"03-02-2023\"),\n",
    "(9,5,6,\"03-03-2023\"),\n",
    "(2,1,12,\"15-06-2023\"),\n",
    "(1,56,2,\"25-06-2023\"),\n",
    "(5,12,5,\"15-04-2023\"),\n",
    "(11,12,76,\"12-03-2023\")]\n",
    "\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "\n",
    "\n",
    "sales_df = spark.createDataFrame(data=sales_data,schema=sales_schema)\n",
    "\n",
    "product_data = [(1, 'fanta',20),\n",
    "(2, 'dew',22),\n",
    "(5, 'sprite',40),\n",
    "(7, 'redbull',100),\n",
    "(12,'mazza',45),\n",
    "(22,'coke',27),\n",
    "(25,'limca',21),\n",
    "(27,'pepsi',14),\n",
    "(56,'sting',10)]\n",
    "\n",
    "product_schema=['id','name','price']\n",
    "\n",
    "product_df = spark.createDataFrame(data=product_data,schema=product_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64cc6b86-0ace-4d61-83ce-6154fab5fa55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          3|       nikita|  delhi|     25-06-2023|\n|          4|        rahul| ranchi|     24-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|\n|          7|        raman|  patna|     30-12-2022|\n|          8|      prakash| ranchi|     24-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|\n+-----------+-------------+-------+---------------+\n\n+-----------+----------+--------+----------------+\n|customer_id|product_id|quantity|date_of_purchase|\n+-----------+----------+--------+----------------+\n|          1|        22|      10|      01-06-2022|\n|          1|        27|       5|      03-02-2023|\n|          2|         5|       3|      01-06-2023|\n|          5|        22|       1|      22-03-2023|\n|          7|        22|       4|      03-02-2023|\n|          9|         5|       6|      03-03-2023|\n|          2|         1|      12|      15-06-2023|\n|          1|        56|       2|      25-06-2023|\n|          5|        12|       5|      15-04-2023|\n|         11|        12|      76|      12-03-2023|\n+-----------+----------+--------+----------------+\n\n+---+-------+-----+\n| id|   name|price|\n+---+-------+-----+\n|  1|  fanta|   20|\n|  2|    dew|   22|\n|  5| sprite|   40|\n|  7|redbull|  100|\n| 12|  mazza|   45|\n| 22|   coke|   27|\n| 25|  limca|   21|\n| 27|  pepsi|   14|\n| 56|  sting|   10|\n+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()\n",
    "sales_df.show()\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6539898-f04a-481b-8d1b-ad363e915aea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4292744384079557>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If youre seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\nFile \u001B[0;32m<command-4292744384079557>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If youre seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'spark'",
       "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'spark'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spark.sql.functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1acdfb-0a42-4804-826c-b0b15fbe7dad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94d04f8-6890-4fab-a296-be532445c548",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fcc242-6159-4f08-b35f-6492dbd58cb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|customer_id|\n+-----------+\n|          1|\n|          1|\n|          1|\n|          2|\n|          2|\n|          5|\n|          5|\n|          7|\n|          9|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"inner\")\\\n",
    "    .select(sales_df[\"customer_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d6bda0-3a61-4fa0-ae6b-000cbfb60aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|product_id|\n+----------+\n|         1|\n|         5|\n|         5|\n|        12|\n|        22|\n|        22|\n|        22|\n|        27|\n|        56|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"inner\")\\\n",
    "    .select(sales_df[\"product_id\"]).sort(\"product_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8936849-eb74-4972-8160-81cc7d0bb274",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n| id|   name|price|\n+---+-------+-----+\n|  1|  fanta|   20|\n|  2|    dew|   22|\n|  5| sprite|   40|\n|  7|redbull|  100|\n| 12|  mazza|   45|\n| 22|   coke|   27|\n| 25|  limca|   21|\n| 27|  pepsi|   14|\n| 56|  sting|   10|\n+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3350f776-fd6b-4201-a7c1-598d8dfa9341",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4292744384079563>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m customer_df\u001B[38;5;241m.\u001B[39mjoin(sales_df,(sales_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m==\u001B[39mcustomer_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m&\u001B[39m (sales_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m==\u001B[39m\u001B[43mcustomer_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mproduct_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m),\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(sales_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39msort(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2918\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   2876\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001B[39;00m\n",
       "\u001B[1;32m   2877\u001B[0m \n",
       "\u001B[1;32m   2878\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2915\u001B[0m \u001B[38;5;124;03m+---+----+\u001B[39;00m\n",
       "\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m-> 2918\u001B[0m     jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_name`, `address`, `date_of_joining`]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4292744384079563>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m customer_df\u001B[38;5;241m.\u001B[39mjoin(sales_df,(sales_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m==\u001B[39mcustomer_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m&\u001B[39m (sales_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m==\u001B[39m\u001B[43mcustomer_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mproduct_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m),\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(sales_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39msort(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2918\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2876\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001B[39;00m\n\u001B[1;32m   2877\u001B[0m \n\u001B[1;32m   2878\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2915\u001B[0m \u001B[38;5;124;03m+---+----+\u001B[39;00m\n\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m-> 2918\u001B[0m     jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_name`, `address`, `date_of_joining`].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_name`, `address`, `date_of_joining`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "customer_df.join(sales_df,(sales_df[\"customer_id\"]==customer_df[\"customer_id\"])& (sales_df[\"product_id\"]==customer_df[\"product_id\"]),\"inner\")\\\n",
    "    .select(sales_df[\"product_id\"]).sort(\"product_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63c8aa0-208a-4d0b-a27b-b7bb33f1b924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4322b0-aa3f-4b36-8896-3fc7a0609f12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+----------------+---+-------+-----+\n|customer_id|product_id|quantity|date_of_purchase| id|   name|price|\n+-----------+----------+--------+----------------+---+-------+-----+\n|          2|         1|      12|      15-06-2023|  1|  fanta|   20|\n|       null|      null|    null|            null|  2|    dew|   22|\n|          9|         5|       6|      03-03-2023|  5| sprite|   40|\n|          2|         5|       3|      01-06-2023|  5| sprite|   40|\n|       null|      null|    null|            null|  7|redbull|  100|\n|         11|        12|      76|      12-03-2023| 12|  mazza|   45|\n|          5|        12|       5|      15-04-2023| 12|  mazza|   45|\n|          7|        22|       4|      03-02-2023| 22|   coke|   27|\n|          5|        22|       1|      22-03-2023| 22|   coke|   27|\n|          1|        22|      10|      01-06-2022| 22|   coke|   27|\n|       null|      null|    null|            null| 25|  limca|   21|\n|          1|        27|       5|      03-02-2023| 27|  pepsi|   14|\n|          1|        56|       2|      25-06-2023| 56|  sting|   10|\n+-----------+----------+--------+----------------+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df,sales_df[\"product_id\"]==product_df[\"id\"],\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c42cde-e617-4260-a964-3d49bf43abf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+-----------+----------+--------+----------------+\n| id|   name|price|customer_id|product_id|quantity|date_of_purchase|\n+---+-------+-----+-----------+----------+--------+----------------+\n|  1|  fanta|   20|          2|         1|      12|      15-06-2023|\n|  2|    dew|   22|       null|      null|    null|            null|\n|  5| sprite|   40|          9|         5|       6|      03-03-2023|\n|  5| sprite|   40|          2|         5|       3|      01-06-2023|\n|  7|redbull|  100|       null|      null|    null|            null|\n| 12|  mazza|   45|         11|        12|      76|      12-03-2023|\n| 12|  mazza|   45|          5|        12|       5|      15-04-2023|\n| 22|   coke|   27|          7|        22|       4|      03-02-2023|\n| 22|   coke|   27|          5|        22|       1|      22-03-2023|\n| 22|   coke|   27|          1|        22|      10|      01-06-2022|\n| 25|  limca|   21|       null|      null|    null|            null|\n| 27|  pepsi|   14|          1|        27|       5|      03-02-2023|\n| 56|  sting|   10|          1|        56|       2|      25-06-2023|\n+---+-------+-----+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.join(sales_df,sales_df[\"product_id\"]==product_df[\"id\"],\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43cad454-d501-46a6-9394-8310df38f28e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f1b7d3-9f76-46ce-99a9-c6a50a664cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          7|        raman|  patna|     30-12-2022|\n|          9|       ragini|kolkata|     03-03-2023|\n+-----------+-------------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5f5865-28b0-4502-9320-70cb797c02af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          3|       nikita|  delhi|     25-06-2023|\n|          4|        rahul| ranchi|     24-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|\n|          8|      prakash| ranchi|     24-02-2023|\n|         10|      raushan| jaipur|     05-02-2023|\n+-----------+-------------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c37531-d3cb-4cc7-9133-faabe10fe0ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f0f41e-c45b-4f33-9b92-5c986ffcdc04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: 100"
     ]
    }
   ],
   "source": [
    "customer_df.crossJoin(sales_df).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8310b86-ad3c-4b48-956d-3eeef4e2a1fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: 10"
     ]
    }
   ],
   "source": [
    "customer_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fa0c9e-a59e-4088-ade3-871d85eac6cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: 10"
     ]
    }
   ],
   "source": [
    "sales_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd28a3b1-4288-4453-bb59-b49452209e32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "emp_schema = ['id','name','sal','dept','gender']\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data,schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7aa2fa-7324-4c42-9897-6b91a1468eb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+\n| id|    name|   sal|     dept|gender|\n+---+--------+------+---------+------+\n|  1|  manish| 50000|       IT|     m|\n|  2|  vikash| 60000|    sales|     m|\n|  3| raushan| 70000|marketing|     m|\n|  4|  mukesh| 80000|       IT|     m|\n|  5|   priti| 90000|    sales|     f|\n|  6|  nikita| 45000|marketing|     f|\n|  7|  ragini| 55000|marketing|     f|\n|  8|   rashi|100000|       IT|     f|\n|  9|  aditya| 65000|       IT|     m|\n| 10|   rahul| 50000|marketing|     m|\n| 11|   rakhi| 50000|       IT|     f|\n| 12|akhilesh| 90000|    sales|     m|\n+---+--------+------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a57c44-92a0-40e6-b24d-271e6345d5fd",
     "showTitle": true,
     "title": "Group by "
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3aa0d71-0d34-44d5-b1f4-945d648aaefa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0309dc39-4f33-4855-a461-c66e62c551aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2820696516126158>:5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m window \u001B[38;5;241m=\u001B[39m Window\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdept\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msal\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 5\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRank\u001B[39m\u001B[38;5;124m\"\u001B[39m,rank()\u001B[38;5;241m.\u001B[39mover(window))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n",
       "\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   4757\u001B[0m     )\n",
       "\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sal` cannot be resolved. Did you mean one of the following? [`id`, `name`, `salary`, `dept`, `gender`].;\n",
       "'Project [id#118L, name#119, salary#120L, gender#122, dept#121, rank() windowspecdefinition(dept#121, 'sal ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS Rank#218]\n",
       "+- Project [id#118L, name#119, salary#120L, gender#122, dept#121]\n",
       "   +- LogicalRDD [id#118L, name#119, salary#120L, dept#121, gender#122], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2820696516126158>:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      3\u001B[0m window \u001B[38;5;241m=\u001B[39m Window\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdept\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msal\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRank\u001B[39m\u001B[38;5;124m\"\u001B[39m,rank()\u001B[38;5;241m.\u001B[39mover(window))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   4757\u001B[0m     )\n\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sal` cannot be resolved. Did you mean one of the following? [`id`, `name`, `salary`, `dept`, `gender`].;\n'Project [id#118L, name#119, salary#120L, gender#122, dept#121, rank() windowspecdefinition(dept#121, 'sal ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS Rank#218]\n+- Project [id#118L, name#119, salary#120L, gender#122, dept#121]\n   +- LogicalRDD [id#118L, name#119, salary#120L, dept#121, gender#122], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sal` cannot be resolved. Did you mean one of the following? [`id`, `name`, `salary`, `dept`, `gender`].;\n'Project [id#118L, name#119, salary#120L, gender#122, dept#121, rank() windowspecdefinition(dept#121, 'sal ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS Rank#218]\n+- Project [id#118L, name#119, salary#120L, gender#122, dept#121]\n   +- LogicalRDD [id#118L, name#119, salary#120L, dept#121, gender#122], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "\n",
    "window = Window.partitionBy(\"dept\").orderBy(\"sal\")\n",
    "\n",
    "emp_df.withColumn(\"Rank\",rank().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c274a5f-aeb6-4c5a-ba93-802f6464cfaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+---------+\n| id|    name|salary|gender|     dept|\n+---+--------+------+------+---------+\n|  1|  manish| 50000|     m|       IT|\n|  2|  vikash| 60000|     m|    sales|\n|  3| raushan| 70000|     m|marketing|\n|  4|  mukesh| 80000|     m|       IT|\n|  5|   priti| 90000|     f|    sales|\n|  6|  nikita| 45000|     f|marketing|\n|  7|  ragini| 55000|     f|marketing|\n|  8|   rashi|100000|     f|       IT|\n|  9|  aditya| 65000|     m|       IT|\n| 10|   rahul| 50000|     m|marketing|\n| 11|   rakhi| 50000|     f|       IT|\n| 12|akhilesh| 90000|     m|    sales|\n+---+--------+------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema = ['id','product_name','sales_date','sales']\n",
    "\n",
    "product_df = spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "emp_schema=['id','name','salary','dept','gender']\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data,schema=emp_schema)\n",
    "emp_df = emp_df.select('id','name','salary','gender','dept')\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "405e9dd5-54b8-4a06-addb-c07487a70bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+-------+\n| id|product_name|sales_date|  sales|\n+---+------------+----------+-------+\n|  1|      iphone|01-01-2023|1500000|\n|  2|     samsung|01-01-2023|1100000|\n|  3|     oneplus|01-01-2023|1100000|\n|  1|      iphone|01-02-2023|1300000|\n|  2|     samsung|01-02-2023|1120000|\n|  3|     oneplus|01-02-2023|1120000|\n|  1|      iphone|01-03-2023|1600000|\n|  2|     samsung|01-03-2023|1080000|\n|  3|     oneplus|01-03-2023|1160000|\n|  1|      iphone|01-04-2023|1700000|\n|  2|     samsung|01-04-2023|1800000|\n|  3|     oneplus|01-04-2023|1170000|\n|  1|      iphone|01-05-2023|1200000|\n|  2|     samsung|01-05-2023| 980000|\n|  3|     oneplus|01-05-2023|1175000|\n|  1|      iphone|01-06-2023|1100000|\n|  2|     samsung|01-06-2023|1100000|\n|  3|     oneplus|01-06-2023|1200000|\n+---+------------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96c4a44-ebf3-4d92-b945-ba8b414c25d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38bf2402-147f-4010-8229-e595414db7c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f44ba43-479e-45e4-9fc0-6a641a9cc720",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+-------+-------------------+\n| id|product_name|sales_date|  sales|privous_month_sales|\n+---+------------+----------+-------+-------------------+\n|  1|      iphone|01-01-2023|1500000|               null|\n|  1|      iphone|01-02-2023|1300000|            1500000|\n|  1|      iphone|01-03-2023|1600000|            1300000|\n|  1|      iphone|01-04-2023|1700000|            1600000|\n|  1|      iphone|01-05-2023|1200000|            1700000|\n|  1|      iphone|01-06-2023|1100000|            1200000|\n|  2|     samsung|01-01-2023|1100000|               null|\n|  2|     samsung|01-02-2023|1120000|            1100000|\n|  2|     samsung|01-03-2023|1080000|            1120000|\n|  2|     samsung|01-04-2023|1800000|            1080000|\n|  2|     samsung|01-05-2023| 980000|            1800000|\n|  2|     samsung|01-06-2023|1100000|             980000|\n|  3|     oneplus|01-01-2023|1100000|               null|\n|  3|     oneplus|01-02-2023|1120000|            1100000|\n|  3|     oneplus|01-03-2023|1160000|            1120000|\n|  3|     oneplus|01-04-2023|1170000|            1160000|\n|  3|     oneplus|01-05-2023|1175000|            1170000|\n|  3|     oneplus|01-06-2023|1200000|            1175000|\n+---+------------+----------+-------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"id\").orderBy(\"sales_date\")\n",
    "\n",
    "last_month_df = product_df.withColumn(\"privous_month_sales\",lag(col(\"sales\"),1).over(window))\n",
    "\n",
    "last_month_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3ac5d5-4f77-4346-bd7d-e4a11b91ec1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+-------+-------------------+\n| id|product_name|sales_date|  sales|privous_month_sales|\n+---+------------+----------+-------+-------------------+\n|  1|      iphone|01-01-2023|1500000|            1300000|\n|  1|      iphone|01-02-2023|1300000|            1600000|\n|  1|      iphone|01-03-2023|1600000|            1700000|\n|  1|      iphone|01-04-2023|1700000|            1200000|\n|  1|      iphone|01-05-2023|1200000|            1100000|\n|  1|      iphone|01-06-2023|1100000|               null|\n|  2|     samsung|01-01-2023|1100000|            1120000|\n|  2|     samsung|01-02-2023|1120000|            1080000|\n|  2|     samsung|01-03-2023|1080000|            1800000|\n|  2|     samsung|01-04-2023|1800000|             980000|\n|  2|     samsung|01-05-2023| 980000|            1100000|\n|  2|     samsung|01-06-2023|1100000|               null|\n|  3|     oneplus|01-01-2023|1100000|            1120000|\n|  3|     oneplus|01-02-2023|1120000|            1160000|\n|  3|     oneplus|01-03-2023|1160000|            1170000|\n|  3|     oneplus|01-04-2023|1170000|            1175000|\n|  3|     oneplus|01-05-2023|1175000|            1200000|\n|  3|     oneplus|01-06-2023|1200000|               null|\n+---+------------+----------+-------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"id\").orderBy(\"sales_date\")\n",
    "\n",
    "last_month_df = product_df.withColumn(\"privous_month_sales\",lead(col(\"sales\"),1).over(window))\n",
    "\n",
    "last_month_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f28d10-0e02-4937-9dc7-35c20fa925f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+-------+-------------------+------------+\n| id|product_name|sales_date|  sales|privous_month_sales|per_los_gain|\n+---+------------+----------+-------+-------------------+------------+\n|  1|      iphone|01-01-2023|1500000|               null|        null|\n|  1|      iphone|01-02-2023|1300000|            1500000|      -15.38|\n|  1|      iphone|01-03-2023|1600000|            1300000|       18.75|\n|  1|      iphone|01-04-2023|1700000|            1600000|        5.88|\n|  1|      iphone|01-05-2023|1200000|            1700000|      -41.67|\n|  1|      iphone|01-06-2023|1100000|            1200000|       -9.09|\n|  2|     samsung|01-01-2023|1100000|               null|        null|\n|  2|     samsung|01-02-2023|1120000|            1100000|        1.79|\n|  2|     samsung|01-03-2023|1080000|            1120000|        -3.7|\n|  2|     samsung|01-04-2023|1800000|            1080000|        40.0|\n|  2|     samsung|01-05-2023| 980000|            1800000|      -83.67|\n|  2|     samsung|01-06-2023|1100000|             980000|       10.91|\n|  3|     oneplus|01-01-2023|1100000|               null|        null|\n|  3|     oneplus|01-02-2023|1120000|            1100000|        1.79|\n|  3|     oneplus|01-03-2023|1160000|            1120000|        3.45|\n|  3|     oneplus|01-04-2023|1170000|            1160000|        0.85|\n|  3|     oneplus|01-05-2023|1175000|            1170000|        0.43|\n|  3|     oneplus|01-06-2023|1200000|            1175000|        2.08|\n+---+------------+----------+-------+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "last_month_df.withColumn(\"per_los_gain\",round(((col(\"sales\")-col('privous_month_sales'))/col(\"sales\"))*100,2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f535a59e-0122-4e54-8080-f5a5ba01cad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.window import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93fa909-c022-4cf5-9f9c-f9b227b8ffe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+\n|product_id|product_name|sales_date|  sales|\n+----------+------------+----------+-------+\n|         2|     samsung|01-01-1995|  11000|\n|         1|      iphone|01-02-2023|1300000|\n|         2|     samsung|01-02-2023|1120000|\n|         3|     oneplus|01-02-2023|1120000|\n|         1|      iphone|01-03-2023|1600000|\n|         2|     samsung|01-03-2023|1080000|\n|         3|     oneplus|01-03-2023|1160000|\n|         1|      iphone|01-01-2006|  15000|\n|         1|      iphone|01-04-2023|1700000|\n|         2|     samsung|01-04-2023|1800000|\n|         3|     oneplus|01-04-2023|1170000|\n|         1|      iphone|01-05-2023|1200000|\n|         2|     samsung|01-05-2023| 980000|\n|         3|     oneplus|01-05-2023|1175000|\n|         1|      iphone|01-06-2023|1100000|\n|         3|     oneplus|01-01-2010|  23000|\n|         2|     samsung|01-06-2023|1100000|\n|         3|     oneplus|01-06-2023|1200000|\n+----------+------------+----------+-------+\n\n+---+------+----------+-----+\n| id|  name|      date| time|\n+---+------+----------+-----+\n|  1|manish|11-07-2023|10:20|\n|  1|manish|11-07-2023|11:20|\n|  2|rajesh|11-07-2023|11:20|\n|  1|manish|11-07-2023|11:50|\n|  2|rajesh|11-07-2023|13:20|\n|  1|manish|11-07-2023|19:20|\n|  2|rajesh|11-07-2023|17:20|\n|  1|manish|12-07-2023|10:32|\n|  1|manish|12-07-2023|12:20|\n|  3|vikash|12-07-2023|09:12|\n|  1|manish|12-07-2023|16:23|\n|  3|vikash|12-07-2023|18:08|\n+---+------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "product_data = [\n",
    "(2,\"samsung\",\"01-01-1995\",11000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-01-2006\",15000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2010\",23000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema=[\"product_id\",\"product_name\",\"sales_date\",\"sales\"]\n",
    "\n",
    "product_df = spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "\n",
    "product_df.show()\n",
    "\n",
    "#Q2.Data:-\n",
    "emp_data = [(1,\"manish\",\"11-07-2023\",\"10:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"11:20\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"11:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"11:50\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"13:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"19:20\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"17:20\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"10:32\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"12:20\"),\n",
    "        (3,\"vikash\",\"12-07-2023\",\"09:12\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"16:23\"),\n",
    "        (3,\"vikash\",\"12-07-2023\",\"18:08\")]\n",
    "\n",
    "emp_schema = [\"id\", \"name\", \"date\", \"time\"]\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "emp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebc97b7-eed1-442e-82cb-7a1ad956512f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"product_id\").orderBy(\"sales_date\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6048aa7f-fda6-47d1-ac73-5f590c6d436c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+-----------------+------------------+\n|product_id|product_name|sales_date|  sales|first_month_sales|latest_month_sales|\n+----------+------------+----------+-------+-----------------+------------------+\n|         1|      iphone|01-01-2006|  15000|            15000|           1100000|\n|         1|      iphone|01-02-2023|1300000|            15000|           1100000|\n|         1|      iphone|01-03-2023|1600000|            15000|           1100000|\n|         1|      iphone|01-04-2023|1700000|            15000|           1100000|\n|         1|      iphone|01-05-2023|1200000|            15000|           1100000|\n|         1|      iphone|01-06-2023|1100000|            15000|           1100000|\n|         2|     samsung|01-01-1995|  11000|            11000|           1100000|\n|         2|     samsung|01-02-2023|1120000|            11000|           1100000|\n|         2|     samsung|01-03-2023|1080000|            11000|           1100000|\n|         2|     samsung|01-04-2023|1800000|            11000|           1100000|\n|         2|     samsung|01-05-2023| 980000|            11000|           1100000|\n|         2|     samsung|01-06-2023|1100000|            11000|           1100000|\n|         3|     oneplus|01-01-2010|  23000|            23000|           1200000|\n|         3|     oneplus|01-02-2023|1120000|            23000|           1200000|\n|         3|     oneplus|01-03-2023|1160000|            23000|           1200000|\n|         3|     oneplus|01-04-2023|1170000|            23000|           1200000|\n|         3|     oneplus|01-05-2023|1175000|            23000|           1200000|\n|         3|     oneplus|01-06-2023|1200000|            23000|           1200000|\n+----------+------------+----------+-------+-----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.withColumn(\"first_month_sales\",first(\"sales\").over(window))\\\n",
    "    .withColumn(\"latest_month_sales\",last(\"sales\").over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cb77b7-65e4-47f1-b9d9-c40ff48803ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = emp_df.withColumn(\"timestamp\",from_unixtime(unix_timestamp(expr(\"CONCAT(DATE,' ',time)\"),\"dd-MM-yyyy HH:mm\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3a24cf-162d-4d8d-9daa-351f601160e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+-------------------+\n| id|  name|      date| time|          timestamp|\n+---+------+----------+-----+-------------------+\n|  1|manish|11-07-2023|10:20|2023-07-11 10:20:00|\n|  1|manish|11-07-2023|11:20|2023-07-11 11:20:00|\n|  2|rajesh|11-07-2023|11:20|2023-07-11 11:20:00|\n|  1|manish|11-07-2023|11:50|2023-07-11 11:50:00|\n|  2|rajesh|11-07-2023|13:20|2023-07-11 13:20:00|\n|  1|manish|11-07-2023|19:20|2023-07-11 19:20:00|\n|  2|rajesh|11-07-2023|17:20|2023-07-11 17:20:00|\n|  1|manish|12-07-2023|10:32|2023-07-12 10:32:00|\n|  1|manish|12-07-2023|12:20|2023-07-12 12:20:00|\n|  3|vikash|12-07-2023|09:12|2023-07-12 09:12:00|\n|  1|manish|12-07-2023|16:23|2023-07-12 16:23:00|\n|  3|vikash|12-07-2023|18:08|2023-07-12 18:08:00|\n+---+------+----------+-----+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23fe179-a014-4918-a08b-4cb9d048dc3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"id\",\"date\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdc3289-0b0a-444c-9429-9e0328569bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2820696516126173>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m new_df \u001B[38;5;241m=\u001B[39m \u001B[43memp_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogin\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mfirst\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtimestamp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mlast\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtimestamp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogin\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mto_timestamp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myyyy-MM-dd HH:mm:ss\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithcolumn\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_time\u001B[39m\u001B[38;5;124m\"\u001B[39m,col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogout\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m-\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogin\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2964\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   2934\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   2935\u001B[0m \n",
       "\u001B[1;32m   2936\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n",
       "\u001B[1;32m   2965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n",
       "\u001B[1;32m   2966\u001B[0m     )\n",
       "\u001B[1;32m   2967\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   2968\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'withcolumn'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-2820696516126173>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m new_df \u001B[38;5;241m=\u001B[39m \u001B[43memp_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogin\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mfirst\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtimestamp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mlast\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtimestamp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogin\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mto_timestamp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myyyy-MM-dd HH:mm:ss\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithcolumn\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_time\u001B[39m\u001B[38;5;124m\"\u001B[39m,col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogout\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m-\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogin\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2964\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   2934\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   2935\u001B[0m \n\u001B[1;32m   2936\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m   2965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n\u001B[1;32m   2966\u001B[0m     )\n\u001B[1;32m   2967\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   2968\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'withcolumn'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'withcolumn'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_df = emp_df.withColumn(\"login\",first(\"timestamp\").over(window))\\\n",
    "    .withColumn(\"logout\",last(\"timestamp\").over(window))\\\n",
    "        .withColumn(\"login\",to_timestamp(\"logout\", \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "            .withcolumn(\"total_time\",col(\"logout\")-col(\"login\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627416ed-d477-46eb-8b62-9fce07a0f000",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3792435141996747#setting/sparkui/0427-033850-chgs5kgl/driver-8837482409791654161\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3792435141996747#setting/sparkui/0427-033850-chgs5kgl/driver-8837482409791654161\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da7827e-fba5-4eeb-8976-75c8ea58d015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b66ba9-1f8f-4108-9f12-0fb5f28ea8c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4260620663083729>:10\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m resturant_json_data \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiline\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferschema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/spark/pratical/resturant_json_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Print the schema of the loaded DataFrame\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#resturant_json_data.printSchema()\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m \n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Explode the 'restaurants' column\u001B[39;00m\n",
       "\u001B[0;32m---> 10\u001B[0m exploded_df \u001B[38;5;241m=\u001B[39m resturant_json_data\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestaurants\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_res\u001B[39m\u001B[38;5;124m\"\u001B[39m))\\\n",
       "\u001B[1;32m     11\u001B[0m     \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestaurants\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m     12\u001B[0m         \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_res.restaurants.R.res_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_res.restaurant.establishment_types\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mest_type_new\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Print the schema of the DataFrame after exploding\u001B[39;00m\n",
       "\u001B[1;32m     16\u001B[0m exploded_df\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [FIELD_NOT_FOUND] No such struct field `restaurants` in `restaurant`."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4260620663083729>:10\u001B[0m\n\u001B[1;32m      1\u001B[0m resturant_json_data \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiline\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferschema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/spark/pratical/resturant_json_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Print the schema of the loaded DataFrame\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#resturant_json_data.printSchema()\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Explode the 'restaurants' column\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m exploded_df \u001B[38;5;241m=\u001B[39m resturant_json_data\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestaurants\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_res\u001B[39m\u001B[38;5;124m\"\u001B[39m))\\\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestaurants\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_res.restaurants.R.res_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_res.restaurant.establishment_types\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mest_type_new\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Print the schema of the DataFrame after exploding\u001B[39;00m\n\u001B[1;32m     16\u001B[0m exploded_df\u001B[38;5;241m.\u001B[39mprintSchema()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [FIELD_NOT_FOUND] No such struct field `restaurants` in `restaurant`.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [FIELD_NOT_FOUND] No such struct field `restaurants` in `restaurant`.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "resturant_json_data = spark.read.format(\"json\")\\\n",
    "    .option(\"multiline\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"/FileStore/spark/pratical/resturant_json_data.json\")\n",
    "\n",
    "# Print the schema of the loaded DataFrame\n",
    "#resturant_json_data.printSchema()\n",
    "\n",
    "# Explode the 'restaurants' column\n",
    "exploded_df = resturant_json_data.select(\"*\", explode(\"restaurants\").alias(\"new_res\"))\\\n",
    "    .drop(\"restaurants\")\\\n",
    ".establishment_types\").alias(\"est_type_new\")).printSchema()\n",
    "        .select(\"*\",\"new_res.restaurants.R.res_id\",explode(\"new_re.restaurant.establishment_types\").alias(\"est_type_new\")).printSchema()\n",
    "\n",
    "\n",
    "# Print the schema of the DataFrame after exploding\n",
    "exploded_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2472a3c5-df96-48a1-b2ed-1509fbbbe671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- restaurants: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- restaurant: struct (nullable = true)\n |    |    |    |-- R: struct (nullable = true)\n |    |    |    |    |-- res_id: long (nullable = true)\n |    |    |    |-- apikey: string (nullable = true)\n |    |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |    |-- cuisines: string (nullable = true)\n |    |    |    |-- currency: string (nullable = true)\n |    |    |    |-- deeplink: string (nullable = true)\n |    |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- featured_image: string (nullable = true)\n |    |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |    |-- has_table_booking: long (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |    |-- location: struct (nullable = true)\n |    |    |    |    |-- address: string (nullable = true)\n |    |    |    |    |-- city: string (nullable = true)\n |    |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |    |-- locality: string (nullable = true)\n |    |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |    |-- zipcode: string (nullable = true)\n |    |    |    |-- menu_url: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- offers: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- photos_url: string (nullable = true)\n |    |    |    |-- price_range: long (nullable = true)\n |    |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |    |-- thumb: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |    |-- votes: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "res_json_data = spark.read.format(\"json\")\\\n",
    "    .option(\"multiline\",\"true\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/resturant_json_data.json\")\\\n",
    "\t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "403dcfc4-ba34-4850-9c7d-86b30f0eff5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2667021414652980>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mres_json_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m,explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestaurants\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_res\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestaurants\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'select'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-2667021414652980>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mres_json_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m,explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestaurants\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_res\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestaurants\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mprintSchema()\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'select'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'select'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_json_data.select(\"*\",explode(\"restaurants\").alias(\"new_res\")).drop(\"restaurants\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa28caa-604f-4985-beb4-5a054272dad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _id: struct (nullable = true)\n |    |-- $oid: string (nullable = true)\n |-- approvalfy: long (nullable = true)\n |-- board_approval_month: string (nullable = true)\n |-- boardapprovaldate: string (nullable = true)\n |-- borrower: string (nullable = true)\n |-- closingdate: string (nullable = true)\n |-- country_namecode: string (nullable = true)\n |-- countrycode: string (nullable = true)\n |-- countryname: string (nullable = true)\n |-- countryshortname: string (nullable = true)\n |-- docty: string (nullable = true)\n |-- envassesmentcategorycode: string (nullable = true)\n |-- grantamt: long (nullable = true)\n |-- ibrdcommamt: long (nullable = true)\n |-- id: string (nullable = true)\n |-- idacommamt: long (nullable = true)\n |-- impagency: string (nullable = true)\n |-- lendinginstr: string (nullable = true)\n |-- lendinginstrtype: string (nullable = true)\n |-- lendprojectcost: long (nullable = true)\n |-- majorsector_percent: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- Name: string (nullable = true)\n |    |    |-- Percent: long (nullable = true)\n |-- mjsector_namecode: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- code: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- mjtheme: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- mjtheme_namecode: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- code: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- mjthemecode: string (nullable = true)\n |-- prodline: string (nullable = true)\n |-- prodlinetext: string (nullable = true)\n |-- productlinetype: string (nullable = true)\n |-- project_abstract: struct (nullable = true)\n |    |-- cdata: string (nullable = true)\n |-- project_name: string (nullable = true)\n |-- projectdocs: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- DocDate: string (nullable = true)\n |    |    |-- DocType: string (nullable = true)\n |    |    |-- DocTypeDesc: string (nullable = true)\n |    |    |-- DocURL: string (nullable = true)\n |    |    |-- EntityID: string (nullable = true)\n |-- projectfinancialtype: string (nullable = true)\n |-- projectstatusdisplay: string (nullable = true)\n |-- regionname: string (nullable = true)\n |-- sector: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- Name: string (nullable = true)\n |-- sector1: struct (nullable = true)\n |    |-- Name: string (nullable = true)\n |    |-- Percent: long (nullable = true)\n |-- sector2: struct (nullable = true)\n |    |-- Name: string (nullable = true)\n |    |-- Percent: long (nullable = true)\n |-- sector3: struct (nullable = true)\n |    |-- Name: string (nullable = true)\n |    |-- Percent: long (nullable = true)\n |-- sector4: struct (nullable = true)\n |    |-- Name: string (nullable = true)\n |    |-- Percent: long (nullable = true)\n |-- sector_namecode: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- code: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- sectorcode: string (nullable = true)\n |-- source: string (nullable = true)\n |-- status: string (nullable = true)\n |-- supplementprojectflg: string (nullable = true)\n |-- theme1: struct (nullable = true)\n |    |-- Name: string (nullable = true)\n |    |-- Percent: long (nullable = true)\n |-- theme_namecode: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- code: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- themecode: string (nullable = true)\n |-- totalamt: long (nullable = true)\n |-- totalcommamt: long (nullable = true)\n |-- url: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "word_json_data = spark.read.format(\"json\")\\\n",
    "    .option(\"multiline\",\"true\")\\\n",
    "\t.option(\"inferSchema\",\"true\")\\\n",
    "\t.load(\"/FileStore/spark/pratical/world_bank-1.json\")\\\n",
    "\t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168731b8-a9f5-4e7b-a11e-b5bfb75730c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4260620663083727>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mword_json_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw_id\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mprintschema()\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'select'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-4260620663083727>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mword_json_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw_id\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mprintschema()\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'select'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'select'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_json_data.select(\".\",explode(\"id\").alias(\"w_id\")).printschema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74ae7cb-d668-48fc-9d7c-1a6f3fa7feb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d873d335-821a-473b-9c58-ec1e9314b48a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_dim_data = [\n",
    "\n",
    "(1,'manish','arwal','india','N','2022-09-15','2022-09-25'),\n",
    "(2,'vikash','patna','india','Y','2023-08-12',None),\n",
    "(3,'nikita','delhi','india','Y','2023-09-10',None),\n",
    "(4,'rakesh','jaipur','india','Y','2023-06-10',None),\n",
    "(5,'ayush','NY','USA','Y','2023-06-10',None),\n",
    "(1,'manish','gurgaon','india','Y','2022-09-25',None),\n",
    "]\n",
    "\n",
    "customer_schema= ['id','name','city','country','active','effective_start_date','effective_end_date']\n",
    "\n",
    "customer_dim_df = spark.createDataFrame(data= customer_dim_data,schema=customer_schema)\n",
    "\n",
    "sales_data = [\n",
    "\n",
    "(1,1,'manish','2023-01-16','gurgaon','india',380),\n",
    "(77,1,'manish','2023-03-11','bangalore','india',300),\n",
    "(12,3,'nikita','2023-09-20','delhi','india',127),\n",
    "(54,4,'rakesh','2023-08-10','jaipur','india',321),\n",
    "(65,5,'ayush','2023-09-07','mosco','russia',765),\n",
    "(89,6,'rajat','2023-08-10','jaipur','india',321)\n",
    "]\n",
    "\n",
    "sales_schema = ['sales_id', 'customer_id','customer_name', 'sales_date', 'food_delivery_address','food_delivery_country', 'food_cost']\n",
    "\n",
    "sales_df = spark.createDataFrame(data=sales_data,schema=sales_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b9606f-2da6-4671-bc1e-5e476f3cc982",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+----------+---------------------+---------------------+---------+\n|sales_id|customer_id|customer_name|sales_date|food_delivery_address|food_delivery_country|food_cost|\n+--------+-----------+-------------+----------+---------------------+---------------------+---------+\n|       1|          1|       manish|2023-01-16|              gurgaon|                india|      380|\n|      77|          1|       manish|2023-03-11|            bangalore|                india|      300|\n|      12|          3|       nikita|2023-09-20|                delhi|                india|      127|\n|      54|          4|       rakesh|2023-08-10|               jaipur|                india|      321|\n|      65|          5|        ayush|2023-09-07|                mosco|               russia|      765|\n|      89|          6|        rajat|2023-08-10|               jaipur|                india|      321|\n+--------+-----------+-------------+----------+---------------------+---------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5600e75-3cd2-4d65-be52-f7c7d1dc76b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+-------+------+--------------------+------------------+\n| id|  name|   city|country|active|effective_start_date|effective_end_date|\n+---+------+-------+-------+------+--------------------+------------------+\n|  1|manish|  arwal|  india|     N|          2022-09-15|        2022-09-25|\n|  2|vikash|  patna|  india|     Y|          2023-08-12|              null|\n|  3|nikita|  delhi|  india|     Y|          2023-09-10|              null|\n|  4|rakesh| jaipur|  india|     Y|          2023-06-10|              null|\n|  5| ayush|     NY|    USA|     Y|          2023-06-10|              null|\n|  1|manish|gurgaon|  india|     Y|          2022-09-25|              null|\n+---+------+-------+-------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_dim_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74c4ae1e-6e72-450b-8634-89606c0954b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_data = customer_dim_df.join(sales_df,customer_dim_df[\"id\"]==sales_df[\"customer_id\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197b3c9d-4270-4494-82b1-bed4518d0353",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>city</th><th>country</th><th>active</th><th>effective_start_date</th><th>effective_end_date</th><th>sales_id</th><th>customer_id</th><th>customer_name</th><th>sales_date</th><th>food_delivery_address</th><th>food_delivery_country</th><th>food_cost</th></tr></thead><tbody><tr><td>1</td><td>manish</td><td>arwal</td><td>india</td><td>N</td><td>2022-09-15</td><td>2022-09-25</td><td>77</td><td>1</td><td>manish</td><td>2023-03-11</td><td>bangalore</td><td>india</td><td>300</td></tr><tr><td>1</td><td>manish</td><td>arwal</td><td>india</td><td>N</td><td>2022-09-15</td><td>2022-09-25</td><td>1</td><td>1</td><td>manish</td><td>2023-01-16</td><td>gurgaon</td><td>india</td><td>380</td></tr><tr><td>2</td><td>vikash</td><td>patna</td><td>india</td><td>Y</td><td>2023-08-12</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>nikita</td><td>delhi</td><td>india</td><td>Y</td><td>2023-09-10</td><td>null</td><td>12</td><td>3</td><td>nikita</td><td>2023-09-20</td><td>delhi</td><td>india</td><td>127</td></tr><tr><td>4</td><td>rakesh</td><td>jaipur</td><td>india</td><td>Y</td><td>2023-06-10</td><td>null</td><td>54</td><td>4</td><td>rakesh</td><td>2023-08-10</td><td>jaipur</td><td>india</td><td>321</td></tr><tr><td>5</td><td>ayush</td><td>NY</td><td>USA</td><td>Y</td><td>2023-06-10</td><td>null</td><td>65</td><td>5</td><td>ayush</td><td>2023-09-07</td><td>mosco</td><td>russia</td><td>765</td></tr><tr><td>1</td><td>manish</td><td>gurgaon</td><td>india</td><td>Y</td><td>2022-09-25</td><td>null</td><td>77</td><td>1</td><td>manish</td><td>2023-03-11</td><td>bangalore</td><td>india</td><td>300</td></tr><tr><td>1</td><td>manish</td><td>gurgaon</td><td>india</td><td>Y</td><td>2022-09-25</td><td>null</td><td>1</td><td>1</td><td>manish</td><td>2023-01-16</td><td>gurgaon</td><td>india</td><td>380</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "manish",
         "arwal",
         "india",
         "N",
         "2022-09-15",
         "2022-09-25",
         77,
         1,
         "manish",
         "2023-03-11",
         "bangalore",
         "india",
         300
        ],
        [
         1,
         "manish",
         "arwal",
         "india",
         "N",
         "2022-09-15",
         "2022-09-25",
         1,
         1,
         "manish",
         "2023-01-16",
         "gurgaon",
         "india",
         380
        ],
        [
         2,
         "vikash",
         "patna",
         "india",
         "Y",
         "2023-08-12",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         3,
         "nikita",
         "delhi",
         "india",
         "Y",
         "2023-09-10",
         null,
         12,
         3,
         "nikita",
         "2023-09-20",
         "delhi",
         "india",
         127
        ],
        [
         4,
         "rakesh",
         "jaipur",
         "india",
         "Y",
         "2023-06-10",
         null,
         54,
         4,
         "rakesh",
         "2023-08-10",
         "jaipur",
         "india",
         321
        ],
        [
         5,
         "ayush",
         "NY",
         "USA",
         "Y",
         "2023-06-10",
         null,
         65,
         5,
         "ayush",
         "2023-09-07",
         "mosco",
         "russia",
         765
        ],
        [
         1,
         "manish",
         "gurgaon",
         "india",
         "Y",
         "2022-09-25",
         null,
         77,
         1,
         "manish",
         "2023-03-11",
         "bangalore",
         "india",
         300
        ],
        [
         1,
         "manish",
         "gurgaon",
         "india",
         "Y",
         "2022-09-25",
         null,
         1,
         1,
         "manish",
         "2023-01-16",
         "gurgaon",
         "india",
         380
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "active",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "effective_start_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "effective_end_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_delivery_address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_delivery_country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_cost",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(join_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0cab3d-7add-4860-8353-23e6b28b4025",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n|customer_id|customer_name|   city|food_delivery_address|active|effective_start_date|effective_end_date|\n+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n|          1|       manish|gurgaon|            bangalore|     N|          2023-03-11|              null|\n|          5|        ayush|     NY|                mosco|     N|          2023-09-07|              null|\n+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "old_records = join_data.where(\n",
    "    (col(\"food_delivery_address\") != col(\"city\")) & (col(\"active\")==\"Y\"))\\\n",
    "        .withColumn(\"active\",lit(\"N\"))\\\n",
    "            .withColumn(\"effective_start_date\",col(\"sales_date\"))\\\n",
    "                .select(\"customer_id\",\n",
    "                        \"customer_name\",\n",
    "                        \"city\",\n",
    "                        \"food_delivery_address\",\n",
    "                        \"active\",\n",
    "                        \"effective_start_date\",\n",
    "                        \"effective_end_date\"\n",
    ")\n",
    "old_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2a76b7f-7afb-4987-9d99-257517af35b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3707071657439826,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_Day_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
